\documentclass{article}
\usepackage{nips15submit_e}

% For figures
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}

% Maths
\usepackage{amsmath}
\usepackage{amsfonts}

% For citations
\usepackage{natbib}

% For links
\usepackage{hyperref}

% For "Jabłoński"
\usepackage[utf8]{inputenc}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}
\newcommand{\msnote}[1]{\textcolor{blue}{[MS: #1]}}


% Header ======================================================================

\title{Ethnicity sensitive author disambiguation of semi-labeled corpora of scientific articles}

\author{Gilles Louppe\\
        CERN\\
        Switzerland\\
\And Hussein Al-Natsheh\\
        CERN\\
        Switzerland\\
\And Mateusz Susik\\
        CERN\\
        Switzerland\\
\And Eamonn Maguire\\
        CERN\\
        Switzerland}
\date{}

\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}

Author name disambiguation in bibliographic databases is the problem of
grouping together scientific publications written by a same person, accounting
for potential homonyms and/or synonyms. Among solutions to this problem,
digital libraries are increasingly offering tools for authors to manually
curate their publications and claim which ones are theirs. Indirectly, these
tools allow for the inexpensive collection of large annotated training data,
which can be further leveraged to build a complementary automated
disambiguation system capable of inferring patterns for identifying
publications written by a same person.  Building upon more than 1 million of
publicly released crowdsourced annotations, we propose an automated author
disambiguation solution exploiting this data (i) to learn an accurate
classifier for identifying corefering authors and (ii) to guide the clustering
of scientific publications by distinct authors in a semi-supervised way. To the
best of our knowledge, our analysis is the first to be carried out on data of
this size and coverage. With respect to the state of the art, we validate the
general pipeline used in most existing solutions, and improve by proposing
phonetic-based blocking strategies, thereby increasing recall, along with strong
ethnicity-sensitive features for learning a linkage function, thereby tailoring
disambiguation to non-Western author names whenever necessary.


\end{abstract}


% Introduction ==================================================================

\section{Introduction}
\label{introduction}

% general introduction to the disambiguation issues
% often based on small dataset or fully unsupervised
% summary of what we did
% outline of the paper

In academic digital libraries, author name disambiguation is the problem of
grouping together publications written by a same person.  Author name
disambiguation is often a difficult problem because an author may use different
spellings or name variants across her career (synonymy) and/or distinct authors may
share the same name (polysemy). Most notably, author disambiguation is often even more
troublesome in the case of researchers from non-Western cultures, where
personal names may be traditionally less diverse (leading to homonym issues) or
for which transliteration to Latin characters may not be unique (leading to
synonym issues). With the fast growth of the scientific literature, author
disambiguation has become a pressing issue because the accuracy of information
managed at the level of individuals directly affects the relevance search of
results (e.g., when querying for all publications written by a given author),
the reliability of bibliometrics and author rankings (e.g., citation counts or other impact
metrics, as studied in \citep{strotmann2012author}) or the relevance of scientific network analysis
\citep{newman2001structure}.

Efforts and solutions to author disambiguation have been proposed from various
communities \citep{liu2014author}. On the one hand, libraries have maintained
authorship control through manual curation, either in a centralized way by
hiring collaborators or by developing services inviting authors to register
their publications themselves (e.g., Google Scholar, Inspire-HEP, ...).
Recently, efforts also started to create persistent digital identifiers
assigned to researchers (e.g., ORCID, ResearchedID, ...), with the further
objective to embed these identifiers in the submission workflow of publishers
or repositories (e.g., Elsevier, arXiv, Inspire-HEP, ...), thereby univocally
solving any disambiguation issue. With the large cost of centralized manual
authorship control, or until the larger adoption of crowdsourced solutions, the
impact of these efforts are unfortunately limited by the efficiency, motivation
and integrity of their active contributors. Similarly, the success of
persistent digital identifier efforts is conditioned to a large and ubiquitous
adoption by both researchers and publishers. For these reasons, fully automated
machine learning-based methods have been proposed during the past decade to
provide immediate, less costly and satisfactory solutions to author
disambiguation. In this work, our goal is to explore and demonstrate how both
approaches can coexist and benefit from each other.  In particular, we study
how labeled data obtained through manual curation (either centralized or
crowdsourced) can be exploited (i) to learn an accurate classifier for
identifying corefering authors and (ii) to guide the clustering of scientific
publications by distinct authors in a semi-supervised way. Our analysis of parameters
and features on this large dataset reveals that the general pipeline
commonly used in existing solutions is an effective approach for author disambiguation.
In addition, and  as validated
by experimental results, we propose alternative strategies for blocking based on the
phonetization of author names, thereby increasing recall. We also propose
effective ethnicity-sensitive features for learning a linkage function,
thereby tailoring author
disambiguation to non-Western author names whenever necessary.

The remaining of this report is structured as follows. In Section~\ref{related-works},
we first briefly review machine learning solutions for author disambiguation.
The components of our method are then defined in Section~\ref{methods}
and its implementation described in Section~\ref{implementation}. Experiments
are carried out in Section~\ref{experiments}, where we explore and validate
features for the supervised learning of a linkage function and compare
strategies for the semi-supervised clustering of publications.
Finally, conclusions and future works are discussed in Section~\ref{conclusions}.


% Related works ==================================================================

\section{Related works}
\label{related-works}

As reviewed in
\citep{smalheiser2009author,ferreira2012brief,levin2012citation}, author
disambiguation algorithms are usually composed of two main components: (i) a
linkage function determining whether two publications have been written by a
same author and (ii) a clustering algorithm producing clusters of publications
assumed to be written by a same author. Approaches can be inventoried along
several axes, depending on the type and amount of data available, the way the
linkage function is learned or defined, or the clustering procedure used to
group publications. Methods relying on supervised learning usually make use of
a small set of
hand-labeled pairs of publications identified as being either from the same or
from different authors for learning automatically a linkage function between
publications \citep{han2004two,huang2006efficient,
culotta2007author,treeratpituk2009disambiguating,tran2014author}. Because
training data is usually not easily available, unsupervised approaches propose
instead to use domain-specific,  manually designed, linkage functions tailored for author
disambiguation \citep{malin2005unsupervised,mcrae2006also,song2007efficient,
soler2007separating, kang2009co,fan2011graph,schulz2014exploiting}. These later
approaches have the advantage of not requiring  hand-labeled data, but often do
not perform as well as the supervised approaches. To reconcile both worlds,
semi-supervised methods make use of small, manually verified, clusters of
publications and/or of high-precision domain-specific rules to build a training
set of pairs of publications, from which a linkage function is then built
using supervised learning
\citep{ferreira2010effective,torvik2009author,levin2012citation}.
Coincidentally, semi-supervised approaches also allow for the tuning of the
clustering algorithm when the later is applied to a mixed set of labeled
and unlabeled publications, e.g., by maximizing some clustering performance
metric on the known clusters \citep{levin2012citation}.

Because of the lack of a large and publicly available dataset of curated
clusters of publications, studies on author disambiguation are usually
constrained to validate their results on manually built datasets of limited
size and scope (from a few hundreds to a few thousands of papers, with a sparse
coverage of ambiguous cases), making the true performance of these methods
often difficult to assess with high confidence. In addition, despite devoted
efforts to construct them, these datasets are very rarely released publicly,
making it even more difficult to compare methods on a common benchmark.

In this context, we position the work presented in this paper as a
semi-supervised  solution for author disambiguation, with the significant
advantage of having a very large collection of more than 1 million crowdsourced annotations
of publications whose true authors are identified. In particular, the extent and coverage
of this data allows us to revisit, validate and nuance previous findings regarding
supervised learning of linkage functions and to better explore strategies
for semi-supervised clustering. Additionally, by releasing the data publicly,
we hope to set a benchmark on which further research on author disambiguation
and related topics can  build upon.



% Methods =====================================================================

\section{Semi-supervised author disambiguation}
\label{methods}

Formally, let us assume a set of publications ${\cal P} = \{ p_0, ...,
p_{N-1}\}$ along with the set of unique individuals ${\cal A} = \{ a_0, ...,
a_{M-1}\}$ having together authored all publications in ${\cal P}$.  Let us
define a signature $s \in p$ from a publication as a unique piece of
information identifying one of the authors of $p$ (e.g., the author name, his
affiliation, along with any other metadata that can be derived from $p$, as illustrated in Figure~\ref{fig:signature}). Let us
denote by ${\cal S} = \{ s | s \in p, p \in {\cal P} \}$ the set of all
signatures that can be extracted from all publications in ${\cal P}$ and
by $S=|{\cal S}|$ the total number of signatures.

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{figures/pub-to-signature}
\caption{An example signature $s$ for "Doe, John". A \textit{signature} is
defined as unique piece of information identifying an author on a publication,
along with any other metadata that can be derived from it, such as publication
title, co-authors or date of publication.}
\label{fig:signature}
\end{figure*}

In this
framework, author disambiguation can be stated as the problem of
finding a partition ${\cal C} = \{ c_0, ..., c_{M-1} \}$ of ${\cal S}$ such
that ${\cal S} = \cup_{i=0}^{M-1} c_i$, $c_i \cap c_j = \phi$ for all $i \neq
j$, and where subsets $c_i$, or clusters, each corresponds to the set of all
signatures belonging to the same individual $a_i$. Alternatively, the set
${\cal A}$ may remain (possibly partially) unknown, such that author
disambiguation boils down to finding a partition ${\cal C}$ where
subsets $c_i$  each corresponds to the set of all signatures from a same
individual (without knowing who). Finally, in the case of partially annotated databases as studied in
this work, the setting extends with the partial knowledge ${\cal C}^\prime = \{ c_0^\prime, ..., c_{M-1}^\prime \}$ of ${\cal C}$,
such that $c_i^\prime \subseteq c_i$, with $c_i^\prime$ possibly empty.
Or put otherwise, the setting extends with the knowledge that all signatures
$s \in c_i^\prime$ belong to the same author.

As inspired from several previous works described in Section~\ref{related-works},
we cast in this work author disambiguation into a semi-supervised clustering
problem.  Our algorithm is made of three parts: (i) a blocking
scheme whose goal is to roughly pre-cluster signatures ${\cal S}$ into smaller groups in order to
reduce computational complexity; (ii) the construction of a linkage function
$d$ between signatures using supervised learning; (iii) the
semi-supervised clustering of all signatures within a same block, using $d$ as a pseudo distance metric.
%
%\begin{figure*}[ht!]
%\centering
%\includegraphics[width=\textwidth]{figures/workflow}
%\caption{Something workflowy...}
%\label{fig:workflow}
%\end{figure*}

\subsection{Blocking}
\label{methods:blocking}

As in previous works, the first part of our algorithm consists in dividing
signatures ${\cal S}$ into disjoint subsets ${\cal S}_{b_0}, ..., {\cal
S}_{b_{K-1}}$, or \textit{blocks} \citep{fellegi69}, and then to carry out
author disambiguation independently on each one of these blocks. In doing so,
computational complexity of clustering (see Section~\ref{methods:clustering})
typically reduces from $O(S^2)$ to $O(\sum_b S_b^2)$, which becomes much more
tractable as the number of signatures increases. Since disambiguation is
performed independently per block, a good blocking strategy should ideally be
designed such that signatures from the same author are all mapped to the same
block, otherwise directly preventing their correct clustering in later stages. As a
result, blocking should balance between reduced complexity and maximum recall.

The simplest and most common strategy for blocking, further denoted to as SFI,
groups signatures together if they share the same surname(s) and the same first
given name initial (e.g., SFI("Doe, John") $==$ "Doe, J"). Despite satisfactory performance, such
a simple strategy fails in several cases, as listed below for
pairs of signatures that should be clustered together but that are put into distinct
blocks:

\begin{enumerate}
  \item There are different
  ways of transcripting an author name or signatures contain a typo
  (e.g., "Mueller, R." and "Muller, R.", "Tchaikovsky, P." and "Czajkowski, P.").

  \item An author has multiple surnames and, on some signatures, all the surnames but the
  last one are contained within given names (e.g., "Martinez Torres, A." and "Torres, A.
  Martinez").

  \item An author has multiple surnames and, on some signatures, only the first surname is
  present (e.g., "Smith-Jones, A." and "Smith, A.")

  \item An author as multiple given names and they are not always all mentioned (e.g.,
  "Smith, Jack" and "Smith, A. J.")

  \item An author changed his surname (e.g., because of marrying somebody).
\end{enumerate}

To account for these issues we propose instead to block signatures based on the
phonetic representation of the normalized surname. Normalization consists in
stripping accents (e.g., "Jabłoński, Ł" $\rightarrow$ "Jablonski, L") and name
affixes that inconsistently appear in signatures (e.g., "van der Waals, J. D."
$\rightarrow$ "Waals, J. D."), while phonetization is based either on the
Double Metaphone \citep{doublemetaphone}, the NYSIIS \citep{nysiis} or the
Soundex \citep{Soundex} algorithms. Together, these processing steps allow to
group most name variants of a same person in the same block while not
significantly increasing computational complexity, thereby solving case 1.

In the case of multiple surnames (cases 2 and 3), we further propose to block
signatures in two phases. In the first phase, all the signatures with a single
surname are clustered together. Every different surname token creates a new
block. In the second phase, the signatures  with multiple surnames are compared
with the blocks for the first and last surname.  If the first surnames of an
author were already used as the last given names on some of the signatures, the
new signature is assigned to the block of the last surname (case 2). Otherwise,
the signature is assigned to the block of the first surname (case 3). Finally,
to prevent the creation of too large blocks, signatures are further divided
along their first given name initial.  Cases 4 and 5 are not
explicitly handled.

%\glnote{I am not sure we need an explicit algorithm, but adding a few examples would help.}

\subsection{Linkage function}
\label{methods:linkage}

\textit{Supervised classification.} The second part of the algorithm is the
automatic construction of a pair-wise linkage function between signatures, for
later use during clustering to group all signatures from a same author.
Formally, the goal is to build a function $d: {\cal S} \times {\cal S} \mapsto
[0, 1]$, such that $d(s_1, s_2)$ tends to $0$ if both signatures $s_1$ and
$s_2$ belong to the same author, and $1$ otherwise. Coincidentally, this
problem can be cast as a standard supervised classification task, where inputs
are pairs of signatures and outputs are classes $0$ (same authors) and $1$
(distinct authors). In this work, we evaluate Random Forests (RF, \cite{breiman2001random}),
Gradient Boosted Regression Trees (GBRT, \cite{friedman2001greedy}) and Logistic Regression \citep{fan2008liblinear} as classifiers.

\textit{Input features.} In most cases, supervised learning algorithms assume
the input space ${\cal X}$ to be numeric (e.g., $\mathbb{R}^p$), making them
not directly applicable to structured input spaces such as ${\cal S} \times
{\cal S}$. Following previous works, pairs of
signatures $(s_1, s_2)$ are first transformed to vectors $v \in \mathbb{R}^p$
by building so-called similarity profiles
\citep{treeratpituk2009disambiguating}, and on which supervised learning is
then carried on. In this work, we design and evaluate 15 standard input
features based on the comparison of signature fields, as reported in the first
half of Table~\ref{table:features}. As an illustrative example, the
\textit{Full name} feature corresponds to the similarity between the (full)
author name fields of the two signatures, as measured using as combination
operator the cosine similarity between their respective $(n,m)$-TF-IDF vector
representations\footnote{$(n,m)$ denotes that the TF-IDF vectors are computed
from character $n$, $n+1$, ..., $m$-grams. When not specified, TF-IDF vectors
are otherwise computed from words.}. Similarly, the \textit{Year difference}
feature measures the absolute difference between the publication date of the
articles to which the two signatures respectively belong.

Observing that author names from different cultures, origins or ethnic groups are likely to be
disambiguated using different strategies (e.g., pairs of signatures with French
author names versus pairs of signatures with Chinese author names), and as
corroborated in \citep{treeratpituk2012name} or \citep{chin2014effective}, we
propose to complement our feature set with 7 additional new features, each
evaluating the degree of belonging of both signatures to several ethnic groups,
as reported in the second half of Table~\ref{table:features}. More
specifically, using census data extracted from \citep{rugglesintegrated}, we
build a logistic regression classifier for mapping the $(1,5)$-TF-IDF
representation of an author name to one of the 7 ethnic groups reported in that
dataset. Given a pair of signatures $(s_1, s_2)$, the proposed ethnicity
features are each computed as the estimated probability of $s_1$ to belong to
the corresponding ethnic group, multiplied by the estimated probability of
$s_2$ to belong to the same group. In doing so, the expectation is for the
linkage function to become sensitive to the actual origin of the authors
depending on the values of these features. Indirectly, let us also note that these
features hold discriminative power since, if author names are predicted
to belong to different ethnic groups, then they are also likely to correspond to
distinct actual persons.

\begin{table}
\caption{Input features for learning a linkage function}
\label{table:features}
\centering
\begin{tabular}{|l|l|}
  \hline
  \textbf{Feature} & \textbf{Combination operator}\\
  \hline
  \hline
  Full name & Cosine similarity of $(2,4)$-TF-IDF\\
  Given names & Cosine similarity of $(2,4)$-TF-IDF\\
  First given name & Jaro-Winkler distance\\
  Second given name & Jaro-Winkler distance\\
  Given name initial & Equality\\
  Affiliation & Cosine similarity of $(2,4)$-TF-IDF\\
  Co-authors & Cosine similarity of TF-IDF\\
  Title & Cosine similarity of $(2,4)$-TF-IDF\\
  Journal & Cosine similarity of $(2,4)$-TF-IDF\\
  Abstract & Cosine similarity of TF-IDF\\
  Keywords & Cosine similarity of TF-IDF\\
  Collaborations & Cosine similarity of TF-IDF\\
  References & Cosine similarity of TF-IDF\\
  Subject & Cosine similarity of TF-IDF\\
  Year difference & Absolute difference\\
  \hline
  White & Product of estimated probabilities\\
  Black & Product of estimated probabilities\\
  American Indian or Alaska Native & Product of estimated probabilities\\
  Chinese & Product of estimated probabilities\\
  Japanese & Product of estimated probabilities\\
  Other Asian or Pacific Islander & Product of estimated probabilities\\
  Others & Product of estimated probabilities\\
  \hline
\end{tabular}
\end{table}

\glnote{Add a few examples of transformed pairs?}

\textit{Building a training set.} The distinctive aspect of our work is the
knowledge of more than 1 million crowdsourced annotations ${\cal C}^\prime = \{
c_0^\prime, ..., c_{M-1}^\prime \}$, indicating together that all signature $s \in
c_i^\prime$ are known to correspond to the same individual $a_i$. In particular,
this data can be used to generate positive pairs $(x=(s_1, s_2), y=0)$ for all
$s_1, s_2 \in c_i^\prime$, for all $i$. Similarly, negative pairs $(x=(s_1,
s_2), y=1)$ can be extracted for all $s_1 \in c_i^\prime, s_2 \in c_j^\prime$, for
all $i \neq j$.

The most straightforward approach for building a training set on which to learn
a linkage function is to sample an equal number of positive and negative pairs,
as suggested above. By observing that the linkage function $d$ will eventually
be used only on pairs of signatures from the same block $S_b$, a further
refinement for building a training set is to restrict  positive and negative
pairs $(s_1, s_2)$ to only those for which $s_1$ and $s_2$ belong to the same
block. In doing so, the trained classifier is forced to learn intra-block
discriminative patterns rather than inter-block differences. Furthermore, as
noted in \citep{lange2011frequency}, pairs of signatures are in the vast
majority non-ambiguous: if both signatures share the same author names, then
they correspond to the same individual, otherwise they do not. Rather than
sampling pairs uniformly at random, we therefore propose to  oversample
difficult cases when building the training set (i.e., pairs of signatures with
different author names corresponding to same individual, and pairs of
signatures with identical author names but corresponding to distinct
individuals) in order to improve the overall accuracy of the linkage function.

\subsection{Semi-supervised clustering}
\label{methods:clustering}

The last component of our author disambiguation pipeline is clustering, that is
the process of grouping together, within a block, all signatures from the same
individual (and only those). As for many other works on author disambiguation,
we make use of hierarchical clustering \citep{ward1963hierarchical} for
building clusters of signatures in a bottom-up fashion. The method consists in
iteratively merging together the two most similar clusters until all clusters
are eventually merged together at the top of the hierarchy. Similarity between
clusters is evaluated using either complete, single or average linkage, using
as pseudo-distance metric the probability that $s_1$ and $s_2$ correspond to
distinct authors, as calculated from the custom linkage function $d$.

To form flat clusters out of the hierarchy, one must decide on a maximum
distance threshold above which clusters are considered to correspond to
distinct authors. Let us denote by ${\cal S}^\prime = \{ s | s \in c^\prime, c^\prime
\in {\cal C}^\prime \}$ the set of all signatures for which partial clusters are
known. Let us also denote by $\smash{\widehat{\cal C}}$  the predicted clusters for all signatures in ${\cal S}$, and by
$\smash{\widehat{\cal C}^\prime} = \{ \widehat{c} \cap {\cal S}^\prime | \widehat{c} \in \widehat{\cal C} \}$
the predicted clusters restricted to signatures for which partial
clusters are known. From these, we evaluate the following semi-supervised cut-off strategies:
\begin{itemize}
\item \textit{No cut:} all signatures from the same block are assumed to be from the same author.
\item \textit{Global cut:} the threshold is chosen globally over all blocks,
    as the one maximizing some score $f({\cal C}^\prime, \widehat{\cal C}^\prime)$.
\item \textit{Block cut:} the threshold is chosen locally at each block $b$,
    as the one maximizing some score $f({\cal C}_b^\prime, \widehat{\cal C}_b^\prime)$.
    In case ${\cal C}_b^\prime$ is empty, then all signatures from $b$ are clustered together.
\end{itemize}

\glnote{Add a figure showing the three possible splits}



% hierarchical clustering
% semi-supervised clustering
% cold start problem (when no labels in a block)

% \subsection{Matching}
% general description in a real system
% GL: out of scope, let us mention that in Conclusions instead


% Implementation ======================================================

\section{Implementation}
\label{implementation}

As part of this work, we developed a stand-alone application for author
disambiguation, publicly available
online\footnote{\url{https://github.com/glouppe/beard}} for free reuse
or study.  Our implementation builds upon the Python scientific stack, making
use of the Scikit-Learn library \citep{scikitlearn} for the supervised learning
of a linkage function and of SciPy \citep{scipy} for clustering. All
components of the disambiguation pipeline have been designed to follow the
Scikit-Learn API \citep{scikitlearnAPI}, making them easy to maintain,
understand and reuse. Our implementation is made to be efficient, exploiting
parallelization when available and making it ready for production environments.


% Results and discussion ======================================================

\section{Experiments}
\label{experiments}

\subsection{Data}

The author disambiguation solution proposed in this work, along with its
enhancements, are evaluated on data extracted from the INSPIRE portal
\citep{gentil2009information}, a digital library for scientific literature in
high-energy physics. Overall, the portal holds more than 1 million
publications ${\cal P}$, forming in total a set ${\cal S}$ of more than 10 millions
signatures. Out of these, around 13\% have been \textit{claimed} by their
original authors or marked as such by professional curators. Together, they
constitute a trusted set $({\cal S}^\prime, {\cal C}^\prime)$ of 15388 distinct individuals sharing
36340 unique author names spread within 1201763 signatures on 360066
publications. This data covers several decades in time and dozens of author
nationalities worldwide.

Following INSPIRE terms of use, the signatures ${\cal S}^\prime$ and their
corresponding clusters ${\cal C}^\prime$ are released
online\footnote{\url{https://github.com/glouppe/paper-author-disambiguation}}
under the CC0 license. To the best of our knowledge, data of this size and
coverage is the first to be publicly released in the scope of author
disambiguation research.

\subsection{Evaluation protocol}

Experiments carried out to study the impact of the proposed algorithmic
components and refinements, as described in Section~\ref{methods}, follow a
standard 3-fold cross-validation protocol, using $({\cal S}^\prime, {\cal
C}^\prime)$ as ground-truth dataset. To replicate the $|{\cal S}^\prime| /
|{\cal S}| \approx 13\%$ ratio of claimed signatures with respect to the total
set of signatures, as on the INSPIRE platform, cross-validation folds are
constructed by sampling 13\% of claimed signatures to form a training set ${\cal
S}_\text{train}^\prime \subseteq {\cal S}^\prime$, and by using the remaining
signatures ${\cal S}_\text{test}^\prime = {\cal S}^\prime \setminus {\cal
S}_\text{train}^\prime$ for testing. Accordingly, ${\cal C}_\text{train}^\prime
= \{ c^\prime \cap {\cal S}_\text{train}^\prime | c^\prime \in {\cal C}^\prime
\}$ represents the partial known clusters on the training fold, while ${\cal
C}_\text{test}^\prime$ are those used for testing.

As commonly done in author disambiguation research,
we evaluate the predicted clusters over testing data  ${\cal C}_\text{test}^\prime$,
using both B3 and pairwise precision, recall
and F-measure, as defined below:
\begin{align}
P_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) &= \frac{1}{|{\cal S}|} \sum_{s \in {\cal S}} \frac{|c(s) \cap \widehat{c}(s)|}{|\widehat{c}(s)|} \\
R_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) &= \frac{1}{|{\cal S}|} \sum_{s \in {\cal S}} \frac{|c(s) \cap \widehat{c}(s)|}{|c(s)|}\\
F_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) &= \frac{2 P_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) R_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S})}{P_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) + P_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S})}\\
P_\text{pairwise}({\cal C}, \widehat{\cal C}) &= \frac{|p({\cal C}) \cap p(\widehat{\cal C})|}{|p(\widehat{\cal C})|}\\
R_\text{pairwise}({\cal C}, \widehat{\cal C}) &= \frac{|p({\cal C}) \cap p(\widehat{\cal C})|}{|p({\cal C})|}\\
F_\text{pairwise}({\cal C}, \widehat{\cal C}) &= \frac{2 P_\text{pairwise}({\cal C}, \widehat{\cal C}) R_\text{pairwise}({\cal C}, \widehat{\cal C})}{P_\text{pairwise}({\cal C}, \widehat{\cal C}) + R_\text{pairwise}({\cal C}, \widehat{\cal C})}
\end{align}
and where $c(s)$ (resp. $\widehat{c}(s)$) is the cluster $c \in {\cal C}$ such that
$s \in c$ (resp. the cluster $\widehat{c} \in \widehat{\cal C}$ such that $s
\in \widehat{c}$), and where $p({\cal C}) = \cup_{c \in {\cal C}} \{ (s_1, s_2)
| s_1, s_2 \in c, s_1 \neq s_2 \}$ is the set of all pairs of signatures from
the same clusters in ${\cal C}$. Intuitively, precision evaluates whether signatures
are grouped only with signatures from the same true clusters, while recall
measures the extent to which all signatures from the same true clusters are
effectively grouped together. The F-measure is the harmonic mean between these
two quantities. In the analysis below, we rely primarily on the B3 F-measure for discussing
results, as the pairwise variant tends to favor large clusters (because the number of pairs is quadratic with the cluster size),
hence unfairly giving preference to authors with many publications. By contrast,
the B3 F-measure weights clusters linearly with respect to their size. General conclusions
drawn below remain however consistent for pairwise F.

\subsection{Results and discussion}

\begin{table}
\caption{Average precision, recall and f-measure scores on test folds.}
\label{table:results}
\centering
\begin{tabular}{|l|c c c|c c c|}
  \hline
                       & \multicolumn{3}{|c|}{\textbf{B3}} & \multicolumn{3}{|c|}{\textbf{Pairwise}} \\
  \textbf{Description} & $P$ & $R$ & $F$ & $P$ & $R$ & $F$\\
  \hline
  \hline
Baseline & 0.9901 & 0.9760 & 0.9830 & 0.9948 & 0.9738 & 0.9842 \\
\hline
\hline
Blocking = SFI & 0.9901 & 0.9760 & 0.9830 & 0.9948 & 0.9738 & 0.9842 \\
Blocking = Double metaphone & 0.9856 & 0.9827 & 0.9841 & 0.9927 & 0.9817 & 0.9871 \\
Blocking = NYSIIS & 0.9875 & 0.9826 & \textbf{0.9850} & 0.9936 & 0.9814 & \textbf{0.9875} \\
Blocking = Soundex & 0.9886 & 0.9745 & 0.9815 & 0.9935 & 0.9725 & 0.9828 \\
\hline
No name normalization & 0.9887 & 0.9697 & 0.9791 & 0.9931 & 0.9658 & 0.9793 \\
Name normalization & 0.9901 & 0.9760 & \textbf{0.9830} & 0.9948 & 0.9738 & \textbf{0.9842} \\
\hline
Classifier = GBRT & 0.9901 & 0.9760 & 0.9830 & 0.9948 & 0.9738 & 0.9842 \\
Classifier = Random Forests & 0.9909 & 0.9783 & \textbf{0.9846} & 0.9957 & 0.9752 & \textbf{0.9854} \\
Classifier = Linear Regression & 0.9749 & 0.9584 & 0.9666 & 0.9717 & 0.9569 & 0.9643 \\
\hline
Training pairs = Non-blocked, uniform & 0.9793 & 0.9630 & 0.9711 & 0.9756 & 0.9629 & 0.9692 \\
Training pairs = Blocked, uniform & 0.9854 & 0.9720 & 0.9786 & 0.9850 & 0.9707 & 0.9778 \\
Training pairs = Blocked, balanced & 0.9901 & 0.9760 & \textbf{0.9830} & 0.9948 & 0.9738 & \textbf{0.9842} \\
\hline
Clustering = Average linkage & 0.9901 & 0.9760 & \textbf{0.9830} & 0.9948 & 0.9738 & \textbf{0.9842} \\
Clustering = Single linkage & 0.9741 & 0.9603 & 0.9671 & 0.9543 & 0.9626 & 0.9584 \\
Clustering = Complete linkage & 0.9862 & 0.9709 & 0.9785 & 0.9920 & 0.9688 & 0.9803 \\
\hline
No cut & 0.9024 & 0.9828 & 0.9409 & 0.8298 & 0.9776 & 0.8977 \\
Global cut & 0.9892 & 0.9737 & 0.9814 & 0.9940 & 0.9727 & 0.9832 \\
Block cut & 0.9901 & 0.9760 & \textbf{0.9830} & 0.9948 & 0.9738 & \textbf{0.9842} \\
  \hline
\end{tabular}
\end{table}

\textit{Baseline.} Results presented in Table~\ref{table:results} are all discussed with
respect to a baseline solution using the following combination of components:
\begin{itemize}
\item Blocking: same surname and the same first given name initial strategy (SFI);
\item Linkage function: all 22 features defined in Table~\ref{table:features},
    gradient boosted regression trees as supervised learning algorithm
    and a training set of pairs built from $({\cal S}_\text{train}^\prime, {\cal C}_\text{train}^\prime)$, by balancing easy and difficult cases.
\item Clustering: agglomerative clustering using average linkage and
    block cuts found to maximize $F_\text{B3}({\cal C}_\text{train}^\prime, \widehat{\cal C}_\text{train}^\prime, {\cal S}_\text{train}^\prime)$.
\end{itemize}


\textit{Blocking.} The very good precision of the baseline ($0.9901$) but its
lower recall ($0.9760$) suggests that the blocking strategy might be the
limiting factor to further overall improvements. As Table~\ref{table:blocking}
indeed indicates, the maximum recall (i.e., if within a block, all signatures
were clustered optimally) for SFI is $0.9828$ , hence not letting much more
room for improvement. At the price of fewer and therefore slightly larger
blocks (as reported in the right column of Table~\ref{table:blocking}), the
proposed phonetic-based blocking strategies show better maximum recall (all
around $0.9905$), thereby pushing further the upper bound on the maximum
performance of author disambiguation. As Table~\ref{table:results} indeed shows,
switching to either Double metaphone or NYSIIS phonetic-based blocking allows
to improve the overall F-measure score, trading precision for recall. In particular,
the NYSIIS-based phonetic blocking shows to be the most effective when applied
to the baseline (with an F-measure of $0.9850$) while also being the most
efficient computationally (with 10857 blocks versus 12978 for the baseline).

Finally, let us also note that Table~\ref{table:blocking} corroborates the
estimation of \citep{torvik2009author}, stating that SFI blocking has a recall
around $98\%$ on real data.

\begin{table}
\caption{Maximum recall of blocking strategies, and their number of blocks on ${\cal S}^\prime$.}
\label{table:blocking}
\centering
\begin{tabular}{|l|cc|c|}
  \hline
  \textbf{Blocking} & $R_\text{B3}^*$ & $R_\text{pairwise}^*$ & \# blocks \\
  \hline
  \hline
    SFI & 0.9828 & 0.9776 & 12978 \\
    Double metaphone & 0.9907 & 0.9863 & 9753 \\
    NYSIIS & 0.9902 & 0.9861 & 10857 \\
    Soundex & 0.9906 & 0.9863 & 9403 \\
  \hline
\end{tabular}
\end{table}

\textit{Name normalization.} As discussed before, the seemingly anodyne step of
normalizing author names (stripping accents, removing affixes), as done in the
baseline, reveals to be quite important. Results of Table~\ref{table:results}
indeed clearly suggests that not normalizing significantly reduces performance
(yielding an F-measure of $0.9830$ when normalizing, but decreasing to $0.9791$
when raw author name strings are used instead).

\textit{Linkage function.} Let us first comment the results regarding the
supervised algorithm used to learn the linkage function. As
Table~\ref{table:results} indicates, both tree-based algorithms appear to be
significantly better fit than Linear Regression ($0.9830$ and $0.9846$ for GBRT
and Random Forests versus $0.9666$ for Linear Regression). This result is
consistent with \citep{treeratpituk2009disambiguating} which evaluated the use
Random Forests for author disambiguation, but contradicts results of
\citep{levin2012citation} for which Logistic Regression appeared to be the best
classifier. Provided hyper-parameters are properly tuned, the superiority of
tree-based methods is in our opinion not surprising. Indeed, given the fact
that the optimal linkage function is likely to be non-linear, non-parametric
methods are expected to yield better results, as the experiments here confirm.

Second, properly constructing a training set of positive and negative pairs of
signatures from which to learn a linkage function reveals to yield quite an
improvement. A random sampling of positive and negative pairs, without taking
blocking into account, significantly impacts the overall performance
($0.9711$). When pairs are drawn only from blocks, performance increases
($0.9786$), which confirms our intuition that $d$ should be built only from
pairs it will be used to eventually cluster. Finally, making the classification
problem more difficult by oversampling complex cases proves to be relevant,
by further improving the disambiguation results ($0.9830$).

\begin{figure}
\centering
\caption{Recursive Feature Elimimation analysis. }
\label{fig:rfe}
\includegraphics[width=\textwidth]{figures/rfe.pdf}
\end{figure}

Using Recursive Feature Elimination \cite{guyon2002gene}, we next evaluate the
usefulness of all 15 standard and 7 additional ethnicity features for learning
the linkage function. The analysis consists in using the baseline algorithm
first using all 22 features, to determine the least discriminative from feature
importances \citep{louppe2013understanding}, and then re-learn the baseline
using all but that one feature. That process is repeated recursively until
eventually only one feature remains. Results are presented in
Figure~\ref{fig:rfe} for one the three folds, starting from the far right with
the baseline and \textit{Second given name} being the least important feature,
and ending on the left with all features eliminated but \textit{Chinese}. As
the figure illustrates, the most important features are ethnic-based features
(\textit{Chinese}, \textit{Other Asian}, \textit{Black}) along with
\textit{Co-authors}, \textit{Affiliation} and \textit{Full name}. Adding the remaining
other features only brings marginal improvements, with \textit{Journal},
\textit{Abstract}, \textit{Collaborations}, \textit{References} \textit{Given
name initial} and \textit{Second given name} being almost insignificant.
Overall, these results highlight the added value of the proposed ehtnic-based
features. Their duality in modeling both the similarity between author names
and their origins make them very strong predictors for author disambiguation.
The results also corroborates results from \citep{kang2009co} or \citep{ferreira2010effective}, who
found that the similarity between co-authors was a highly discriminative
feature. It indeed appears in 4th in our ranking and brings very significant
improvements as the figure illustrates. In case computational complexity is a concern, this analysis also
underlines the fact that decent performance can be achieved using only a very
small set of features, as also observed in
\citep{treeratpituk2009disambiguating} or \citep{levin2012citation}.

\textit{Semi-supervised clustering.} The last part of our experiments concerns
the study of agglomerative clustering and the best way to find a cut-off
threshold to form clusters. Results from Table~\ref{table:results}
first clearly indicates that average linkage is significantly better than
both single and complete linkage.

Clustering together all signatures from the same block is the least effective
strategy ($0.9409$), but yields anyhow surprisingly decent accuracy, given the
fact it requires almost no computation (i.e., both learning a linkage function
and running agglomerative clustering can be skipped -- only the blocking
function is needed to group signatures). In particular, this result reveals
that author names are not ambiguous in most cases\footnote{This holds for the
data we extracted, but may in the future, with the rise of non-Western
researchers, be an underestimate of the ambiguous cases.} and that only a small
fraction of them requires advanced disambiguation procedures. On the other
hand, both global and block cut thresholding strategies give very good results,
with a slight advantage for block cuts ($0.9814$ versus $0.9830$), as expected.
In case ${\cal S}^\prime_b$ is empty (e.g., because it corresponds to a young
researcher at the beginning of his career), this therefore suggests that using
a cut-off threshold learned globally from the known data would in general give
satisfactory results, only marginally worse than if claimed signatures had been
known.


% Conclusions =================================================================

\section{Conclusions}
\label{conclusions}

% future improvements


\glnote{Mention Hussein's thesis for further experiments}
\glnote{Virtuous circle}


% References ==================================================================

\bibliographystyle{apalike}
\bibliography{bib}

\end{document}
