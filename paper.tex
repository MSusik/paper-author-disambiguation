\documentclass{article}
%\usepackage[margin=1.2in]{geometry}

\usepackage{nips15submit_e}


% For figures
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% For links
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\glnote}[1]{\textcolor{red}{GL: #1}}


% Header ======================================================================

\title{Semi-supervised author disambiguation of scientific publications}

\author{Gilles Louppe\\
        CERN\\
        Switzerland\\
\And Hussein Al-Natsheh\\
        CERN\\
        Switzerland\\
\And Mateusz Susik\\
        CERN\\
        Switzerland\\
\And Eamonn Maguire\\
        CERN\\
        Switzerland}
\date{}

\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}

Author name disambiguation in bibliographic databases is the problem of
grouping together scientific publications written by a same person, accounting
for potential homonyms and/or synonyms. Among solutions ot this problem, digital
libraries are increasingly offering tools for authors to manually curate their
publications and claim which ones are theirs. Indirectly, these tools allow for
the inexpensive collection of large annotated training data, which can be
further leveraged to build an automated disambiguation system capable of
inferring features or patterns for identifying publications written by a same
person.  In this context, we propose a semi-supervised machine learning
disambiguation solution in the case large and partially annotated databases of
scientific publications. Using a real-world dataset of 1 million manual
annotations, we demonstrate how we exploited this data (i) to learn an accurate
classifier for identifying corefering authors, (ii) to guide the clustering of
scientific publications by distinct authors in a semi-supervised way and
finally (iii) to match the resulting clusters with existing profiles.  Our
analysis of parameters and features on this large training set reveal that...
\glnote{To be completed with actual results.}

\end{abstract}


% Introduction ==================================================================

\section{Introduction}
\label{introduction}

% general introduction to the disambiguation issues
% often based on small dataset or fully unsupervised
% summary of what we did
% outline of the paper

In academic digital libraries, author name disambiguation is the problem of
grouping together publications written by a same person.  With the fast growth
of the scientific literature, this has become a pressing issue because the
accuracy of information managed at the level of individuals directly affects
the relevance search results (e.g., when querying for all publications written
by a given author), the reliability of bibliometrics (e.g., citation counts or
other impact metrics) or the relevance of scientific network analysis. Author
name disambiguation is often a difficult problem because an author may use
different spellings, variants or synonyms across her career and/or distinct
authors may share the same name. In particular, author disambiguation becomes
even more critical for researchers from Eastern cultures, where personal names
may be traditionally less diverse (leading to homonym issues) or for which
transliteration to latin characters may not be unique (leading to synonym
issues).

Efforts and solutions to author disambiguation have been proposed from various
communities \citep{liu2014author}. On the one hand, libraries have maintained
authorship control through manual curation, either in a centralized way by
hiring collaborators or by developing services inviting authors to register
their publications themselves (e.g., Google Scholar, Inspire-HEP). Recently,
efforts also started to assign persistent digital identifiers to researchers
(e.g., ORCID, ResearchedID), with the further objective to embed these
identifiers in the submission workflow of publishers or repositeries (e.g.,
Elsevier, arXiv, Inspire-HEP), thereby univocally solving any disambiguation
issue. With the large cost of centralized manual authorship control or until
the larger adoption of crowdsourced solutions, the impact of these efforts are
unfortunately limited by the efficiency, motivation or integrity of their
active contributors. Similarly, the success of persistent digital identifier
efforts is conditioned to a large and ubiquituous adoption by both reseachers and publishers.
For these reasons, fully automated machine learning-based methods have been
proposed during the past decade \glnote{Add references} to provide immediate, less costly
and efficient solutions to author disambiguation. In this work, our goal
is to explore and demonstrate how both approaches can coexist and benefit from
each other.  In particular, we study how curated data obtained through manual
curation (either centralized or crowdsourced) can be exploited (i) to learn an
accurate classifier for identifying corefering authors, (ii) to guide the
clustering of scientific publications by distinct authors in a semi-supervised
way and finally (iii) to match the resulting clusters with existing profiles.
Our analysis of parameters and features on this large training set reveal
that... \glnote{To be completed with actual results.}

The remaining of this report is structured as follows. In Section~\ref{related-works}, ...


% Related works ==================================================================

\section{Related works}
\label{related-works}

% levin

% Methods =====================================================================

\section{Semi-supervised author disambiguation}
\label{methods}

\subsection{Overview}
% figure, notations, etc

\subsection{Distance learning}
% distance learning as a supervised problem
% balancing difficult and easy cases

\subsection{Blocking}
% motivation
% last name, first initial
% phonetics something (dm)

\subsection{Semi-supervised clustering}
% hierarchical clustering
% semi-supervised clustering
% cold start problem (when no labels in a block)

\subsection{Matching}
% general description in a real system


% Implementation ======================================================

\section{Implementation}
\label{implementation}


% Results and discussion ======================================================

\section{Experiments}
\label{experiments}

\subsection{Data}
% source of the data, labels in the data, etc
% release the data

\subsection{Evaluation protocol}
% measures (b^3, paired, size of clusters)
% training / valid / test sets

\subsection{Distance learning}
% feature engineering
% variable importance analysis
% supervised learning model

\subsection{Blocking}

\subsection{Semi-supervised clustering}
% compare agglomerative algorithms
% compare threshold strategies
% compare cold start solutions
% talk about impact of small improvements in terms of citations statistics


% Conclusions =================================================================

\section{Conclusions}
\label{conclusions}

% future improvements


% References ==================================================================

\bibliographystyle{apalike}
\bibliography{bib}

\end{document}
