\documentclass{article}
%\usepackage[margin=1.2in]{geometry}

\usepackage{nips15submit_e}


% For figures
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}

% For links
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For "Jabłoński"
\usepackage[utf8]{inputenc}

% For placing tables in correct place
\usepackage{placeins}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}
\newcommand{\msnote}[1]{\textcolor{blue}{[MS: #1]}}


% Header ======================================================================

\title{Author disambiguation of large-scale semi-labeled collections of scientific articles}

\author{Gilles Louppe\\
        CERN\\
        Switzerland\\
\And Hussein Al-Natsheh\\
        CERN\\
        Switzerland\\
\And Mateusz Susik\\
        CERN\\
        Switzerland\\
\And Eamonn Maguire\\
        CERN\\
        Switzerland}
\date{}

\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}

Author name disambiguation in bibliographic databases is the problem of
grouping together scientific publications written by a same person, accounting
for potential homonyms and/or synonyms. Among solutions to this problem,
digital libraries are increasingly offering tools for authors to manually
curate their publications and claim which ones are theirs. Indirectly, these
tools allow for the inexpensive collection of large annotated training data,
which can be further leveraged to build a complementary automated disambiguation system
capable of inferring patterns for identifying publications written by a same
person.  Building upon more than 1 million crowdsourced annotations, we
propose an automated author disambiguation solution exploiting this data (i) to learn
an accurate classifier for identifying corefering authors, (ii) to guide the
clustering of scientific publications by distinct authors in a semi-supervised
way and finally (iii) to match the resulting clusters with existing profiles.
To the best of our knowledge, our analysis is the first to be carried out on
data of this size and coverage. It reveals that... \glnote{To be completed with
actual results.}

\end{abstract}

\glnote{Note sure if we should talk about matching in this paper?}
\glnote{Insist on the complementarity of having both crowdsourced and automated disambiguation.}


% Introduction ==================================================================

\section{Introduction}
\label{introduction}

% general introduction to the disambiguation issues
% often based on small dataset or fully unsupervised
% summary of what we did
% outline of the paper

In academic digital libraries, author name disambiguation is the problem of
grouping together publications written by a same person.  Author name
disambiguation is often a difficult problem because an author may use different
spellings or name variants across her career (synonymy) and/or distinct authors may
share the same name (polysemy). Most notably, author disambiguation is often even more
troublesome in the case of researchers from non-Western cultures, where
personal names may be traditionally less diverse (leading to homonym issues) or
for which transliteration to latin characters may not be unique (leading to
synonym issues). With the fast growth of the scientific literature, author
disambiguation has become a pressing issue because the accuracy of information
managed at the level of individuals directly affects the relevance search of
results (e.g., when querying for all publications written by a given author),
the reliability of bibliometrics and author rankings (e.g., citation counts or other impact
metrics, as studied in \citep{strotmann2012author}) or the relevance of scientific network analysis.
\glnote{Add more references.}

Efforts and solutions to author disambiguation have been proposed from various
communities \citep{liu2014author}. On the one hand, libraries have maintained
authorship control through manual curation, either in a centralized way by
hiring collaborators or by developing services inviting authors to register
their publications themselves (e.g., Google Scholar, Inspire-HEP, ...).
Recently, efforts also started to create persistent digital identifiers
assigned to researchers (e.g., ORCID, ResearchedID, ...), with the further
objective to embed these identifiers in the submission workflow of publishers
or repositeries (e.g., Elsevier, arXiv, Inspire-HEP, ...), thereby univocally
solving any disambiguation issue. With the large cost of centralized manual
authorship control, or until the larger adoption of crowdsourced solutions, the
impact of these efforts are unfortunately limited by the efficiency, motivation
and integrity of their active contributors. Similarly, the success of
persistent digital identifier efforts is conditioned to a large and ubiquituous
adoption by both researchers and publishers. For these reasons, fully automated
machine learning-based methods have been proposed during the past decade to
provide immediate, less costly and efficient solutions to author
disambiguation. In this work, our goal is to explore and demonstrate how both
approaches can coexist and benefit from each other.  In particular, we study
how labeled data obtained through manual curation (either centralized or
crowdsourced) can be exploited (i) to learn an accurate classifier for
identifying corefering authors, (ii) to guide the clustering of scientific
publications by distinct authors in a semi-supervised way and finally (iii) to
match the resulting clusters with existing profiles. Our analysis of parameters
and features on this large training set reveals that... \glnote{To be completed
with actual results.} \glnote{Also mention error analysis, suggesting that
uncertain disambiguation results should be suggested to human curators, thereby
closing the loop and creating a vertuous circle.}

The remaining of this report is structured as follows. In Section~\ref{related-works},
we first briefly review machine learning solutions for author disambiguation.
The components of our method are then defined in Section~\ref{methods}
and its implementation described in Section~\ref{implementation}. Experiments
are carried out in Section~\ref{experiments}, where we explore and validate
features for the supervised learning of a similarity function and compare
strategies for the semi-supervised clustering of publications.
Finally, conclusions and future works are discussed in Section~\ref{conclusions}.


% Related works ==================================================================

\section{Related works}
\label{related-works}

As reviewed in
\citep{smalheiser2009author,ferreira2012brief,levin2012citation}, author
disambiguation algorithms are usually composed of two main components: (i) a
similarity function determining whether two publications have been written by a
same author and (ii) a clustering algorithm producing clusters of publications
assumed to be written by a same author. Approaches can be inventorized along
several axes, depending on the type and amount of data available, the way the
similarity function is learned or defined, or the clustering procedure used to
group publications. Methods relying on supervised learning usually make use of
a small set of
hand-labeled pairs of publications identified as being either from the same or
from different authors for learning automatically a similarity function between
publications \citep{han2004two,huang2006efficient,
culotta2007author,treeratpituk2009disambiguating,tran2014author}. Because
training data is often not easily available, unsupervised approaches propose
instead to use domain-specific similarity functions tailored for author
disambiguation \citep{malin2005unsupervised,mcrae2006also,song2007efficient,
soler2007separating, kang2009co,fan2011graph,schulz2014exploiting}. These later
approaches have the advantage of not requiring  hand-labeled data, but often do
not perform as well as the supervised approaches. To reconcile both worlds,
semi-supervised methods make use of small, manually verified, clusters of
publications and/or of high-precision domain-specific rules to build a training
set of pairs of publications, from which a similarity function is then built
using supervised learning
\citep{ferreira2010effective,torvik2009author,levin2012citation}.
Coincidentally, semi-supervised approaches also allow for the tuning of the
clustering algorithm when the later is applied to a mixed set of labeled
and unlabeled publications, e.g., by maximizing some clustering performance
metric on the known clusters \citep{levin2012citation}.

Because of the lack of a large and publicly available dataset of curated
clusters of publications, studies on author disambiguation are usually
constrained to validate their results on manually built datasets of limited
size and scope (from a few hundreds to a few thousands of papers, with a sparse
coverage of ambiguous cases), making the true performance of these methods
often difficult to assess with high confidence. In addition, despite devoted
efforts to construct them, these datasets are very rarely released publicly,
making it even more difficult to compare methods on a common benchmark.

In this context, we position the work presented in this paper as a
semi-supervised  solution for author disambiguation, but with the significant
advantage of having a very large collection of more than 1 million crowdsourced annotations
of publications whose true authors are identified. In particular, the extent and coverage
of this data allows us to revisit, validate and nuance previous findings regarding
supervised learning of similarity functions and to better explore strategies
for semi-supervised clustering. Additionally, by releasing the data publicly,
we hope to set a benchmark on which further research on author disambiguation
and related topics can  build upon.



% Methods =====================================================================

\section{Semi-supervised author disambiguation}
\label{methods}

\subsection{Overview}

Formally, let us assume a set of publications ${\cal P} = \{ p_0, ...,
p_{N-1}\}$ along with the set of unique individuals ${\cal A} = \{ a_0, ...,
a_{M-1}\}$ having together authored all publications in ${\cal P}$.  Let us
define a signature $s \in p$ from a publication as a unique piece of
information identifying one of the authors of $p$ (e.g., the author name, his
affiliation, along with any other metadata that can be derived from $p$). Let us
denote by ${\cal S} = \{ s | s \in p, p \in {\cal P} \}$ the set of all
signatures that can be extracted from all publications in ${\cal P}$. In this
framework, author disambiguation can be stated as the problem of
finding a partition ${\cal C} = \{ c_0, ..., c_{M-1} \}$ of ${\cal S}$ such
that ${\cal S} = \cup_{i=0}^{M-1} c_i$, $c_i \cap c_j = \phi$ for all $i \neq
j$, and where subsets $c_i$, or clusters, each corresponds to the set of all
signatures belonging to the same individual $a_i$. Alternatively, the set
${\cal A}$ may remain (possibly partially) unknown, such that author
disambiguation boils down to finding a partition ${\cal C}$ where
subsets $c_i$  each corresponds to the set of all signatures from a same
individual. Finally, in the case of partially annotated databases as studied in
this work, the setting extends with the partial knowledge of pairs $(a, s) \in
{\cal A} \times {\cal S}$, each indicating that signature $s$ is known with high confidence
to belong to individual $a$.

In this work, author disambiguation is cast into a semi-supervised clustering
problem, as inspired from several previous works described in Section~\ref
{related-works}. Our algorithm is made of four elements: (i) a blocking
function $b$ whose goal is to rhoughly pre-cluster signatures ${\cal S}$ into smaller groups in order to
reduce computational complexity; (ii) the construction of a similarity function
$d$ between signatures using supervised learning; (iii) the
semi-supervised clustering of all signatures within a same block, using $d$; (iv) the matching
of predicted clusters to existing profiles in a database.

\glnote{Figure required here (Eamonn)}

\subsection{Blocking}

If the goal of the algorithm is to disambiguate millions of signatures and the algorithm
complexity is $\Omega{(n^2)}$ in terms of the number of signatures due to pairwise
comparisons, the disambiguation might take weeks or months to finish. In order to prevent
that, we \textit{block} the signatures \citep{fellegi69}. The simplest strategy groups the
signatures together if they share the same surnames and first given name initial (e.g.,
"Wang, Z"). As the clustering will be performed only within the
blocks, such strategy might prevent the further steps of the algorithm from assigning
correctly two signatures to the same author. We extracted from our training set all pairs
where the signatures belong to different blocks and the same author after the last name and
first initial strategy was used. We observed that we can classify majority of these pairs
into few categories:
\begin{enumerate}
  \item Author has multiple surnames and on some of his signatures all the surnames but the
  last one are contained within given names (e.g., "Martinez Torres, A." and "Torres, A.
  Martinez").
  \item Author has multiple surnames and on some of his signatures only the first surname is
  present (e.g., "Smith-Jones, A." and "Smith, A.")
  \item Author comes from a country that doesn't use latin alphabet and there are different
  ways of transcripting his name or his name sometimes contains a typo on the signatures
  (e.g., "Mueller, R." and "Muller, R.", "Tchaikovsky, P." and "Czajkowski, P.").
  \item Author sometimes uses on the signature a different name than his first one (e.g.,
  "Smith, Jack" and "Smith, A.J.")
  \item Author changed his surname (e.g., because of marrying somebody).
\end{enumerate}

We tackled first four categories\msnote{Should I mention why not all of them?} and
implemented a blocking strategy that produces less pairs with signatures from the same
author but different blocks, and the blocks output by the algorithm are small. Before
performing blocking, the names on the signatures are \textit{normalized}: the accents are
removed (e.g., "Jabłoński, Ł" $\rightarrow$ "Jablonski, L") and common affixes are removed
(e.g., "van der Waals, J. D."  $\rightarrow$ "Waals, J. D.")\msnote{Better explanation of
"affix" needed?}.
\begin{algorithm}
  \caption{Blocking using phonetic algorithm}
  \begin{algorithmic}[1]
    \newcommand{\myindent}[1]{
        \newline\makebox[#1cm]{}
    }
    \Require{$S$ - set of signatures. The names on the signatures should be
             \textit{normalized},
             \myindent{0}$phonetic\_surnames$ - functions that outputs phonetic
             representation of surnames from \myindent{1.38}given signature,
             \myindent{0}$given\_names$ - function that ouputs all the given names from
             given signature.}

    \Function{Phonetic blocking}{$S$}
      \State $blocks\gets \emptyset$
      \Comment{Blocks are indexed by a phonetic representation of a surname}
      \ForAll{$s$ in $S$ with single surname}
         \myindent{0.92}assign $s$ to the block of $phonetic\_surnames(s)[0]$
      \EndFor
      \ForAll{$s$ in $S$ with multiple surnames}
         \If{block of $phonetic\_surnames(s)[-1]$ contains a signature $t$
         \myindent{1.84}where $phonetic\_surnames(s)[0] = given\_names(t)[-1]$}
         \myindent{1.38}assign $s$ to the block of $phonetc\_surnames(s)[-1]$
         \ElsIf{block of $phonetic\_surnames(s)[0]$ exists}
          \myindent{1.38}assign $s$ to the block of $phonetic\_surnames(s)[0]$
         \Else
          \myindent{1.38}assign $s$ to the block of $phonetic\_surnames(s)[-1]$
         \EndIf
      \EndFor
      \Return $blocks$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\msnote{Should I explain that if the block does not exist, an empty one is created?}

After this algorithm is finished, all the blocks from the result of the function above can
be additionally split over the first given name initial.

\subsection{Learning a similarity function}
\glnote{By Gilles}

% distance learning as a supervised problem
% balancing difficult and easy cases
% sampling pairs from block only

\subsection{Semi-supervised clustering}
% hierarchical clustering
% semi-supervised clustering
% cold start problem (when no labels in a block)

\subsection{Matching}
% general description in a real system


% Implementation ======================================================

\section{Implementation}
\label{implementation}

\glnote{By Mateusz}
\msnote{If the matching is not described in this paper, change appearances of "four elements"}

Having the algorithm split into four elements, we implemented each of them separately.
Learning a similarity function and semi-supervised clustering steps required sophisticated
machine learning algorithms which implementations transcended the scope of this project.
Thus, we used ready-to-go \textit{Scikit-learn} library \citep{scikit-learn} and
semi-supervised clustering from \textit{SciPy} \citep{scipy}. In order to keep the code
maintainable and abstract, we followed practices introduces by \textit{Scikit-learn API}
\citep{scikit-learn-API}.

Performance-wise, the clustering step appeared to be the bottleneck. We improved the
algorithmic efficiency by parallelizing it. Having $n$ cores available, one can run the
semi-supervised clustering for $n$ different blocks in the same moment. In the production
systems the disambiguation often need not be run on the whole dataset. Once the dataset has
been disambiguated once, it is possible to avoid rerunning the whole algorithm when the new
signatures arrive in the digital library. First of all, the similarity function can be
reused. Moreover, the clustering has to be applied only to the blocks the new signatures are
contained within.

\msnote{Fill the table with times}
\msnote{Add times for clustering with phonetic blockings if they prove to be better}

\FloatBarrier
\begin{table}[H]
\caption{Time performance (2 GHz per CPU core, 32GB RAM)}
\centering
\begin{tabular}{|l|l|}
  \hline
  \textbf{algorithm} & \textbf{time}\\
  \hline
  blocking 1200000 signatures & dfsf \\
  \hline
  learning similarity function on 1000 pairs on reduced set of features & w1k2\\
  \hline
  learning similarity function on 1000000 pairs on reduced set of features & w2k2 \\
  \hline
  learning similarity function on 1000000 pairs on full set of features & sdfsdf \\
  \hline
  clustering 1200000 signatures on reduced set of features using 16 cores & asdsda \\
  \hline
  clustering 1200000 signatures on reduced set of features using 1 cores & aasdfs \\
  \hline
  clustering 1200000 signatures on full set of features using 16 cores & ssesdf \\
  \hline
  matching clusters from 1200000 signatures with the old onews & asdsd \\
  \hline
\end{tabular}
\end{table}



% Results and discussion ======================================================

\section{Experiments}
\label{experiments}

\subsection{Data}

\glnote{By Gilles}

% source of the data, labels in the data, etc
% release the data

\subsection{Evaluation protocol}

% measures (b^3, paired, size of clusters)
% training / valid / test sets

\subsection{Base experiment}

The starting point of our analysis is an algorithm that blocks the signatures using the last
name and first initial strategy, samples the pairs in balanced way, uses gradient boosting
with all features included for learning the similarity function and clusters using
semi-supervised hierarchical approach.

\FloatBarrier
\begin{table}[H]
\caption{Base experiment evaluation)}
\centering
\begin{tabular}{|r|r|l|l|l|}
  \hline
  Evaluation & Set & Precision & Recall & F-score \\
  \hline
  Pairwise & test & 0.9948 & 0.9738 & 0.9842 \\
  \hline
  Pairwise & train & 0.9976 & 0.9747 & 0.986 \\
  \hline
  Pairwise & overall & 0.9952 & 0.9739 & 0.9844 \\
  \hline
  $B^3$ & test & 0.9901 & 0.976 & 0.983 \\
  \hline
  $B^3$ & train & 0.997 & 0.979 & 0.9879 \\
  \hline
  $B^3$ & overall & 0.9907 & 0.9761 & 0.9834 \\
  \hline
\end{tabular}
\end{table}

\subsection{Blocking}

Let us introduce a term \textit{blocking limited maximum recall} to describe the highest
recall that the algorithm can theoretically reach after blocking is done.
Formally, for the set of signatures ${\cal S}$, blocking function $F: \mathcal{S}
\rightarrow \mathcal{B}$, where $ \mathcal{B} = \{s_{0}, \ldots{} , s_{N-1}\}$ is a
partition of $\mathcal{S} $ , the recall function $\mathcal{R}: \mathcal{C}_{gold},
\mathcal{C}_{predicted} \rightarrow \langle0,1\rangle$ where $\mathcal{C}_{gold}$ is the
partition of $\mathcal{S}$ where for every author there is a set in $\mathcal{C}_{gold}$
that contains only all the signatures of this author, $\mathcal{C}_{predicted}$ is the
partition of $\mathcal{S}$ output by the algorithm, and $\mathcal{Q} = \{\cup_{i=0}^{N-1}
q_i $ : $\forall_{i \in \langle0,N-1\rangle} q_i$ is a partition of $s_i$\}. Then
\textit{blocking limited maximum recall} is defined as $\max_{q \in \mathcal{Q}}
\mathcal{R}(\mathcal{C}_{gold}, q)$. For sanely defined recall functions, such as $B^{3}$
recall or \textit{pairwise} recall \msnote{Further explanation needed, why it is
equivalent?}, it is equivalent to $\mathcal{R}(\mathcal{C}_{gold}, \mathcal{B})$.

The \textit{blocking limited maximum recall} for the last name and first initial blocking
and $B^3$ scoring equals 0.9827. Comparing it to the $B^3$ recall of the base experiment,
one can see that a huge part of errors comes from the blocking step which forbids some of
the pairs to be merged together. To prevent that, we tried the phonetic blocking algorithm
using three different phonetic algorithms: Double Metaphone \citep{doublemetaphone}, NYSIIS
\citep{nysiis} and Soundex \citep{Soundex}. As Double Metaphone can return two results for a
string, we always discarded the second one when we used this algorithm.

\FloatBarrier
\begin{table}[H]
\caption{Blocking limited maximum recalls (BLMR) on the whole dataset without splitting over
the first given name initial}
\centering
\begin{tabular}{|r|l|l|}
  \hline
  blocking function & BLMR with $B^{3}$ recall & BLMR with pairwise recall \\
  \hline
  last name and first initial & 0.9827 & 0.9776 \\
  \hline
  phonetic using double metaphone & 0.9967 & 0.9953 \\
  \hline
  phonetic using nysiis & 0.9961 & 0.9951 \\
  \hline
  phonetic using soundex & 0.9966 & 0.9953 \\
  \hline
\end{tabular}
\end{table}

\msnote{Mention two new features for this experiments, other than that everything is the
same as in the base experiment}

\FloatBarrier
\begin{table}[H]
\caption{Phonetic algorithm performance}
\centering
\begin{tabular}{|r|r|r|l|l|l|}
  \hline
  Phonetic Algorithm & Split over initial & Evaluation & Precision & Recall & F-score \\
  \hline
  Double Metaphone & yes & pairwise & 0.9934 & 0.982 & 0.9876 \\
  \hline
  Double Metaphone & yes & $B^3$ & 0.9867 & 0.983 & 0.9848 \\
  \hline
  Double Metaphone & no & pairwise & - & - & - \\
  \hline
  Double Metaphone & no & $B^3$ & - & - & - \\
  \hline
  NYSIIS & yes & pairwise & 0.9942 & 0.9818 & 0.9879 \\
  \hline
  NYSIIS & yes & $B^3$ & 0.9884 & 0.9828 & 0.9856 \\
  \hline
  NYSIIS & no & pairwise & - & - & - \\
  \hline
  NYSIIS & no & $B^3$ & - & - & - \\
  \hline
  Soundex & yes & pairwise & - & - & - \\
  \hline
  Soundex & yes & $B^3$ & - & - & - \\
  \hline
  Soundex & no & pairwise & - & - & - \\
  \hline
  Soundex & no & $B^3$ & - & - & - \\
  \hline
\end{tabular}
\end{table}

\msnote{Fill the table}

% compare with torvik & smalheiser (2009) who argue that lnfi blocking has recall around 98%
% MS - recall for lnfi (blocking only) 0.9776, lnfi (whole algorithm) 0.9739,
% MS - all three results use pairwise scoring

\subsection{Learning a similarity function}
% feature engineering
% variable importance analysis
% supervised learning model

\subsection{Semi-supervised clustering}
% compare agglomerative algorithms
% compare threshold strategies
% compare cold start solutions


% Conclusions =================================================================

\section{Conclusions}
\label{conclusions}

% future improvements


% References ==================================================================

\bibliographystyle{apalike}
\bibliography{bib}

\end{document}
