\documentclass{article}
%\usepackage[margin=1.2in]{geometry}

\usepackage{nips15submit_e}


% For figures
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% For links
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}
\newcommand{\msnote}[1]{\textcolor{blue}{[MS: #1]}}


% Header ======================================================================

\title{Author disambiguation of large-scale semi-labeled collections of scientific articles}

\author{Gilles Louppe\\
        CERN\\
        Switzerland\\
\And Hussein Al-Natsheh\\
        CERN\\
        Switzerland\\
\And Mateusz Susik\\
        CERN\\
        Switzerland\\
\And Eamonn Maguire\\
        CERN\\
        Switzerland}
\date{}

\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}

Author name disambiguation in bibliographic databases is the problem of
grouping together scientific publications written by a same person, accounting
for potential homonyms and/or synonyms. Among solutions to this problem,
digital libraries are increasingly offering tools for authors to manually
curate their publications and claim which ones are theirs. Indirectly, these
tools allow for the inexpensive collection of large annotated training data,
which can be further leveraged to build a complementary automated disambiguation system
capable of inferring patterns for identifying publications written by a same
person.  Building upon more than 1 million crowdsourced annotations, we
propose a automated author disambiguation solution exploiting this data (i) to learn
an accurate classifier for identifying corefering authors, (ii) to guide the
clustering of scientific publications by distinct authors in a semi-supervised
way and finally (iii) to match the resulting clusters with existing profiles.
To the best of our knowledge, our analysis is the first to be carried out on
data of this size and coverage. It reveals that... \glnote{To be completed with
actual results.}

\end{abstract}

\glnote{Note sure if we should talk about matching in this paper?}
\glnote{Insist on the complementarity of having both crowdsourced and automated disambiguation.}


% Introduction ==================================================================

\section{Introduction}
\label{introduction}

% general introduction to the disambiguation issues
% often based on small dataset or fully unsupervised
% summary of what we did
% outline of the paper

In academic digital libraries, author name disambiguation is the problem of
grouping together publications written by a same person.  Author name
disambiguation is often a difficult problem because an author may use different
spellings or name variants across her career (synonymy) and/or distinct authors may
share the same name (polysemy). Most notably, author disambiguation is often even more
troublesome in the case of researchers from non-Western cultures, where
personal names may be traditionally less diverse (leading to homonym issues) or
for which transliteration to latin characters may not be unique (leading to
synonym issues). With the fast growth of the scientific literature, author
disambiguation has become a pressing issue because the accuracy of information
managed at the level of individuals directly affects the relevance search of
results (e.g., when querying for all publications written by a given author),
the reliability of bibliometrics and author rankings (e.g., citation counts or other impact
metrics, as studied in \citep{strotmann2012author}) or the relevance of scientific network analysis.
\glnote{Add more references.}

Efforts and solutions to author disambiguation have been proposed from various
communities \citep{liu2014author}. On the one hand, libraries have maintained
authorship control through manual curation, either in a centralized way by
hiring collaborators or by developing services inviting authors to register
their publications themselves (e.g., Google Scholar, Inspire-HEP, ...).
Recently, efforts also started to create persistent digital identifiers
assigned to researchers (e.g., ORCID, ResearchedID, ...), with the further
objective to embed these identifiers in the submission workflow of publishers
or repositeries (e.g., Elsevier, arXiv, Inspire-HEP, ...), thereby univocally
solving any disambiguation issue. With the large cost of centralized manual
authorship control, or until the larger adoption of crowdsourced solutions, the
impact of these efforts are unfortunately limited by the efficiency, motivation
and integrity of their active contributors. Similarly, the success of
persistent digital identifier efforts is conditioned to a large and ubiquituous
adoption by both researchers and publishers. For these reasons, fully automated
machine learning-based methods have been proposed during the past decade to
provide immediate, less costly and efficient solutions to author
disambiguation. In this work, our goal is to explore and demonstrate how both
approaches can coexist and benefit from each other.  In particular, we study
how labeled data obtained through manual curation (either centralized or
crowdsourced) can be exploited (i) to learn an accurate classifier for
identifying corefering authors, (ii) to guide the clustering of scientific
publications by distinct authors in a semi-supervised way and finally (iii) to
match the resulting clusters with existing profiles. Our analysis of parameters
and features on this large training set reveals that... \glnote{To be completed
with actual results.} \glnote{Also mention error analysis, suggesting that
uncertain disambiguation results should be suggested to human curators, thereby
closing the loop and creating a vertuous circle.}

The remaining of this report is structured as follows. In Section~\ref{related-works},
we first briefly review machine learning solutions for author disambiguation.
The components of our method are then defined in Section~\ref{methods}
and its implementation described in Section~\ref{implementation}. Experiments
are carried out in Section~\ref{experiments}, where we explore and validate
features for the supervised learning of a similarity function and compare
strategies for the semi-supervised clustering of publications.
Finally, conclusions and future works are discussed in Section~\ref{conclusions}.


% Related works ==================================================================

\section{Related works}
\label{related-works}

As reviewed in
\citep{smalheiser2009author,ferreira2012brief,levin2012citation}, author
disambiguation algorithms are usually composed of two main components: (i) a
similarity function determining whether two publications have been written by a
same author and (ii) a clustering algorithm producing clusters of publications
assumed to be written by a same author. Approaches can be inventorized along
several axes, depending on the type and amount of data available, the way the
similarity function is learned or defined, or the clustering procedure used to
group publications. Methods relying on supervised learning usually make use of
a small set of
hand-labeled pairs of publications identified as being either from the same or
from different authors for learning automatically a similarity function between
publications \citep{han2004two,huang2006efficient,
culotta2007author,treeratpituk2009disambiguating,tran2014author}. Because
training data is often not easily available, unsupervised approaches propose
instead to use domain-specific similarity functions tailored for author
disambiguation \citep{malin2005unsupervised,mcrae2006also,song2007efficient,
soler2007separating, kang2009co,fan2011graph,schulz2014exploiting}. These later
approaches have the advantage of not requiring  hand-labeled data, but often do
not perform as well as the supervised approaches. To reconcile both worlds,
semi-supervised methods make use of small, manually verified, clusters of
publications and/or of high-precision domain-specific rules to build a training
set of pairs of publications, from which a similarity function is then built
using supervised learning
\citep{ferreira2010effective,torvik2009author,levin2012citation}.
Coincidentally, semi-supervised approaches also allow for the tuning of the
clustering algorithm when the later is applied to a mixed set of labeled
and unlabeled publications, e.g., by maximizing some clustering performance
metric on the known clusters \citep{levin2012citation}.

Because of the lack of a large and publicly available dataset of curated
clusters of publications, studies on author disambiguation are usually
constrained to validate their results on manually built datasets of limited
size and scope (from a few hundreds to a few thousands of papers, with a sparse
coverage of ambiguous cases), making the true performance of these methods
often difficult to assess with high confidence. In addition, despite devoted
efforts to construct them, these datasets are very rarely released publicly,
making it even more difficult to compare methods on a common benchmark.

In this context, we position the work presented in this paper as a
semi-supervised  solution for author disambiguation, but with the significant
advantage of having a very large collection of more than 1 million crowdsourced annotations
of publications whose true authors are identified. In particular, the extent and coverage
of this data allows us to revisit, validate and nuance previous findings regarding
supervised learning of similarity functions and to better explore strategies
for semi-supervised clustering. Additionally, by releasing the data publicly,
we hope to set a benchmark on which further research on author disambiguation
and related topics can  build upon.



% Methods =====================================================================

\section{Semi-supervised author disambiguation}
\label{methods}

\subsection{Overview}

Formally, let us assume a set of publications ${\cal P} = \{ p_0, ...,
p_{N-1}\}$ along with the set of unique individuals ${\cal A} = \{ a_0, ...,
a_{M-1}\}$ having together authored all publications in ${\cal P}$.  Let us
define a signature $s \in p$ from a publication as a unique piece of
information identifying one of the authors of $p$ (e.g., the author name, his
affiliation, along with any other metadata that can be derived from $p$). Let us
denote by ${\cal S} = \{ s | s \in p, p \in {\cal P} \}$ the set of all
signatures that can be extracted from all publications in ${\cal P}$. In this
framework, author disambiguation can be stated as the problem of
finding a partition ${\cal C} = \{ c_0, ..., c_{M-1} \}$ of ${\cal S}$ such
that ${\cal S} = \cup_{i=0}^{M-1} c_i$, $c_i \cap c_j = \phi$ for all $i \neq
j$, and where subsets $c_i$, or clusters, each corresponds to the set of all
signatures belonging to the same individual $a_i$. Alternatively, the set
${\cal A}$ may remain (possibly partially) unknown, such that author
disambiguation boils down to finding a partition ${\cal C}$ where
subsets $c_i$  each corresponds to the set of all signatures from a same
individual. Finally, in the case of partially annotated databases as studied in
this work, the setting extends with the partial knowledge of pairs $(a, s) \in
{\cal A} \times {\cal S}$, each indicating that signature $s$ is known with high confidence
to belong to individual $a$.

In this work, author disambiguation is cast into a semi-supervised clustering
problem, as inspired from several previous works described in Section~\ref
{related-works}. Our algorithm is made of four elements: (i) a blocking
function $b$ whose goal is to rhoughly pre-cluster signatures ${\cal S}$ into smaller groups in order to
reduce computational complexity; (ii) the construction of a similarity function
$d$ between signatures using supervised learning; (iii) the
semi-supervised clustering of all signatures within a same block, using $d$; (iv) the matching
of predicted clusters to existing profiles in a database.

\glnote{Figure required here (Eamonn)}

\subsection{Blocking}

\glnote{By Mateusz}

% motivation
% last name, first initial
% phonetics something (dm)

\subsection{Learning a similarity function}

\glnote{By Gilles}

% distance learning as a supervised problem
% balancing difficult and easy cases
% sampling pairs from block only

\subsection{Semi-supervised clustering}
% hierarchical clustering
% semi-supervised clustering
% cold start problem (when no labels in a block)

\subsection{Matching}
% general description in a real system


% Implementation ======================================================

\section{Implementation}
\label{implementation}

\glnote{By Mateusz}

% cite sklearn paper
% cite sklearn api paper


% Results and discussion ======================================================

\section{Experiments}
\label{experiments}

\subsection{Data}

\glnote{By Gilles}

% source of the data, labels in the data, etc
% release the data

\subsection{Evaluation protocol}

% measures (b^3, paired, size of clusters)
% training / valid / test sets

\subsection{Blocking}
% compare with torvik & smalheiser (2009) who argue that lnfi blocking has recall around 98%

\subsection{Learning a similarity function}
% feature engineering
% variable importance analysis
% supervised learning model

\subsection{Semi-supervised clustering}
% compare agglomerative algorithms
% compare threshold strategies
% compare cold start solutions


% Conclusions =================================================================

\section{Conclusions}
\label{conclusions}

% future improvements


% References ==================================================================

\bibliographystyle{apalike}
\bibliography{bib}

\end{document}
