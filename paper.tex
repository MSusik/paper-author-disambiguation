\documentclass{article}
%\usepackage[margin=1.2in]{geometry}

\usepackage{nips15submit_e}


% For figures
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}
\usepackage{amsmath}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}

% For links
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For "Jabłoński"
\usepackage[utf8]{inputenc}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}
\newcommand{\msnote}[1]{\textcolor{blue}{[MS: #1]}}


% Header ======================================================================

\title{Author disambiguation of large-scale semi-labeled collections of scientific articles}

\author{Gilles Louppe\\
        CERN\\
        Switzerland\\
\And Hussein Al-Natsheh\\
        CERN\\
        Switzerland\\
\And Mateusz Susik\\
        CERN\\
        Switzerland\\
\And Eamonn Maguire\\
        CERN\\
        Switzerland}
\date{}

\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}

Author name disambiguation in bibliographic databases is the problem of
grouping together scientific publications written by a same person, accounting
for potential homonyms and/or synonyms. Among solutions to this problem,
digital libraries are increasingly offering tools for authors to manually
curate their publications and claim which ones are theirs. Indirectly, these
tools allow for the inexpensive collection of large annotated training data,
which can be further leveraged to build a complementary automated
disambiguation system capable of inferring patterns for identifying
publications written by a same person.  Building upon more than 1 million
crowdsourced annotations, we propose an automated author disambiguation
solution exploiting this data (i) to learn an accurate classifier for
identifying corefering authors and (ii) to guide the clustering of scientific
publications by distinct authors in a semi-supervised way. To the best of our
knowledge, our analysis is the first to be carried out on data of this size and
coverage. It reveals that... \glnote{To be completed with actual results.}

\end{abstract}

\glnote{Insist on the complementarity of having both crowdsourced and automated disambiguation.}


% Introduction ==================================================================

\section{Introduction}
\label{introduction}

% general introduction to the disambiguation issues
% often based on small dataset or fully unsupervised
% summary of what we did
% outline of the paper

In academic digital libraries, author name disambiguation is the problem of
grouping together publications written by a same person.  Author name
disambiguation is often a difficult problem because an author may use different
spellings or name variants across her career (synonymy) and/or distinct authors may
share the same name (polysemy). Most notably, author disambiguation is often even more
troublesome in the case of researchers from non-Western cultures, where
personal names may be traditionally less diverse (leading to homonym issues) or
for which transliteration to latin characters may not be unique (leading to
synonym issues). With the fast growth of the scientific literature, author
disambiguation has become a pressing issue because the accuracy of information
managed at the level of individuals directly affects the relevance search of
results (e.g., when querying for all publications written by a given author),
the reliability of bibliometrics and author rankings (e.g., citation counts or other impact
metrics, as studied in \citep{strotmann2012author}) or the relevance of scientific network analysis.
\glnote{Add more references.}

Efforts and solutions to author disambiguation have been proposed from various
communities \citep{liu2014author}. On the one hand, libraries have maintained
authorship control through manual curation, either in a centralized way by
hiring collaborators or by developing services inviting authors to register
their publications themselves (e.g., Google Scholar, Inspire-HEP, ...).
Recently, efforts also started to create persistent digital identifiers
assigned to researchers (e.g., ORCID, ResearchedID, ...), with the further
objective to embed these identifiers in the submission workflow of publishers
or repositeries (e.g., Elsevier, arXiv, Inspire-HEP, ...), thereby univocally
solving any disambiguation issue. With the large cost of centralized manual
authorship control, or until the larger adoption of crowdsourced solutions, the
impact of these efforts are unfortunately limited by the efficiency, motivation
and integrity of their active contributors. Similarly, the success of
persistent digital identifier efforts is conditioned to a large and ubiquituous
adoption by both researchers and publishers. For these reasons, fully automated
machine learning-based methods have been proposed during the past decade to
provide immediate, less costly and efficient solutions to author
disambiguation. In this work, our goal is to explore and demonstrate how both
approaches can coexist and benefit from each other.  In particular, we study
how labeled data obtained through manual curation (either centralized or
crowdsourced) can be exploited (i) to learn an accurate classifier for
identifying corefering authors and (ii) to guide the clustering of scientific
publications by distinct authors in a semi-supervised way. Our analysis of parameters
and features on this large training set reveals that... \glnote{To be completed
with actual results.} \glnote{Also mention error analysis, suggesting that
uncertain disambiguation results should be suggested to human curators, thereby
closing the loop and creating a vertuous circle.}

The remaining of this report is structured as follows. In Section~\ref{related-works},
we first briefly review machine learning solutions for author disambiguation.
The components of our method are then defined in Section~\ref{methods}
and its implementation described in Section~\ref{implementation}. Experiments
are carried out in Section~\ref{experiments}, where we explore and validate
features for the supervised learning of a linkage function and compare
strategies for the semi-supervised clustering of publications.
Finally, conclusions and future works are discussed in Section~\ref{conclusions}.


% Related works ==================================================================

\section{Related works}
\label{related-works}

As reviewed in
\citep{smalheiser2009author,ferreira2012brief,levin2012citation}, author
disambiguation algorithms are usually composed of two main components: (i) a
linkage function determining whether two publications have been written by a
same author and (ii) a clustering algorithm producing clusters of publications
assumed to be written by a same author. Approaches can be inventorized along
several axes, depending on the type and amount of data available, the way the
linkage function is learned or defined, or the clustering procedure used to
group publications. Methods relying on supervised learning usually make use of
a small set of
hand-labeled pairs of publications identified as being either from the same or
from different authors for learning automatically a linkage function between
publications \citep{han2004two,huang2006efficient,
culotta2007author,treeratpituk2009disambiguating,tran2014author}. Because
training data is often not easily available, unsupervised approaches propose
instead to use domain-specific linkage functions tailored for author
disambiguation \citep{malin2005unsupervised,mcrae2006also,song2007efficient,
soler2007separating, kang2009co,fan2011graph,schulz2014exploiting}. These later
approaches have the advantage of not requiring  hand-labeled data, but often do
not perform as well as the supervised approaches. To reconcile both worlds,
semi-supervised methods make use of small, manually verified, clusters of
publications and/or of high-precision domain-specific rules to build a training
set of pairs of publications, from which a linakge function is then built
using supervised learning
\citep{ferreira2010effective,torvik2009author,levin2012citation}.
Coincidentally, semi-supervised approaches also allow for the tuning of the
clustering algorithm when the later is applied to a mixed set of labeled
and unlabeled publications, e.g., by maximizing some clustering performance
metric on the known clusters \citep{levin2012citation}.

Because of the lack of a large and publicly available dataset of curated
clusters of publications, studies on author disambiguation are usually
constrained to validate their results on manually built datasets of limited
size and scope (from a few hundreds to a few thousands of papers, with a sparse
coverage of ambiguous cases), making the true performance of these methods
often difficult to assess with high confidence. In addition, despite devoted
efforts to construct them, these datasets are very rarely released publicly,
making it even more difficult to compare methods on a common benchmark.

In this context, we position the work presented in this paper as a
semi-supervised  solution for author disambiguation, but with the significant
advantage of having a very large collection of more than 1 million crowdsourced annotations
of publications whose true authors are identified. In particular, the extent and coverage
of this data allows us to revisit, validate and nuance previous findings regarding
supervised learning of linkage functions and to better explore strategies
for semi-supervised clustering. Additionally, by releasing the data publicly,
we hope to set a benchmark on which further research on author disambiguation
and related topics can  build upon.



% Methods =====================================================================

\section{Semi-supervised author disambiguation}
\label{methods}

\subsection{Overview}

Formally, let us assume a set of publications ${\cal P} = \{ p_0, ...,
p_{N-1}\}$ along with the set of unique individuals ${\cal A} = \{ a_0, ...,
a_{M-1}\}$ having together authored all publications in ${\cal P}$.  Let us
define a signature $s \in p$ from a publication as a unique piece of
information identifying one of the authors of $p$ (e.g., the author name, his
affiliation, along with any other metadata that can be derived from $p$, as illustrated in Figure~\ref{fig:signature}). Let us
denote by ${\cal S} = \{ s | s \in p, p \in {\cal P} \}$ the set of all
signatures that can be extracted from all publications in ${\cal P}$ and
by $S=|{\cal S}|$ the total number of signatures.

\begin{figure}
\label{fig:signature}
\begin{tabular}{ l l l }
  $s:$ & Title & Lorem ipsum dolor sit amet, consectetur adipiscing elit \\
  & Author & Doe, John \\
  & Affiliation & University of Foo \\
  & Co-authors & Smith, John; Chen, Wang\\
  & Year & 2015\\
\end{tabular}

\caption{An example of signature $s$ for "Doe, John". A \textit{signature} is
defined as unique piece of information identifying an author on a publication,
along with any other metadata that can be derived from it, such as publication
title, co-authors or date of publication.}
\end{figure}

In this
framework, author disambiguation can be stated as the problem of
finding a partition ${\cal C} = \{ c_0, ..., c_{M-1} \}$ of ${\cal S}$ such
that ${\cal S} = \cup_{i=0}^{M-1} c_i$, $c_i \cap c_j = \phi$ for all $i \neq
j$, and where subsets $c_i$, or clusters, each corresponds to the set of all
signatures belonging to the same individual $a_i$. Alternatively, the set
${\cal A}$ may remain (possibly partially) unknown, such that author
disambiguation boils down to finding a partition ${\cal C}$ where
subsets $c_i$  each corresponds to the set of all signatures from a same
individual. Finally, in the case of partially annotated databases as studied in
this work, the setting extends with the partial knowledge of pairs $(a, s) \in
{\cal A} \times {\cal S}$, each indicating that signature $s$ is known with high confidence
to belong to individual $a$.

As inspired from several previous works described in Section~\ref{related-works},
we cast in this work author disambiguation into a semi-supervised clustering
problem.  Our algorithm is made of three parts: (i) a blocking
scheme whose goal is to rhoughly pre-cluster signatures ${\cal S}$ into smaller groups in order to
reduce computational complexity; (ii) the construction of a linkage function
$d$ between signatures using supervised learning; (iii) the
semi-supervised clustering of all signatures within a same block, using $d$ as a pseudo distance metric.

\glnote{Figure required here (Eamonn)}

\subsection{Blocking}
\label{methods:blocking}

As in previous works, the first part of our algorithm consists in dividing
signatures ${\cal S}$ into disjoint subsets ${\cal S}_{b_0}, ..., {\cal
S}_{b_{K-1}}$, or \textit{blocks} \citep{fellegi69}, and then to carry out
author disambiguation independently on each one of these blocks. In doing so,
computational complexity of clustering (see Section~\ref{methods:clustering})
typically reduces from $O(S^2)$ to $O(\sum_b S_b^2)$, which becomes much more
tractable as the number of signatures increases. Since disambiguation is
performed independently per block, a good blocking strategy should ideally be
designed such that signatures from the same author are all mapped to the same
block, otherwise directly preventing their correct clustering in later stages. As a
result, blocking should balance between reduced complexity and maximum recall.

The simplest and most common strategy for blocking \glnote{add references}
groups signatures together if they share the same surname(s) and the same first
given name initial (e.g., "Doe, J"). Despite satisfying performance, such
a simple strategy fails in several cases, as listed below for
pairs of signatures that should be clustered together but that are put into distinct
blocks:

\begin{enumerate}
  \item There are different
  ways of transcripting an author name or signatures contain a typo
  (e.g., "Mueller, R." and "Muller, R.", "Tchaikovsky, P." and "Czajkowski, P.").

  \item An author has multiple surnames and, on some signatures, all the surnames but the
  last one are contained within given names (e.g., "Martinez Torres, A." and "Torres, A.
  Martinez").

  \item An author has multiple surnames and, on some signatures, only the first surname is
  present (e.g., "Smith-Jones, A." and "Smith, A.")

  \item An author as multiple given names and they are not always all mentioned (e.g.,
  "Smith, Jack" and "Smith, A. J.")

  \item An author changed his surname (e.g., because of marrying somebody).
\end{enumerate}

To account for these issues we propose instead to block signatures based on the
phonetic representation of the normalized surname. Normalization consists in
stripping accents (e.g., "Jabłoński, Ł" $\rightarrow$ "Jablonski, L") and name
affixes that inconsistenly appear in signatures (e.g., "van der Waals, J. D."
$\rightarrow$ "Waals, J. D."), while phonetization is based either on the
Double Metaphone \citep{doublemetaphone}, the NYSIIS \citep{nysiis} or the
Soundex \citep{Soundex} algorithms. Together, these processing steps allow to
group most name variants of a same person in the same block while not
significantly increasing computational complexity, thereby solving case 1.

In the case of multiple surnames (cases 2 and 3), we further propose to block
signatures in two phases. In the first phase, all the signatures with  one
surname are clustered together. Every different surname token creates a new
block. In the second phase, the signatures  with multiple surnames are compared
with the blocks for the first and last surname.  If the first surnames of an
author were already used as the last given names on some of the signatures, the
new signature is assigned to the block of the last surname (case 2). Otherwise,
the signature is assigned to the block of the first surname (case 3). Finally,
to prevent the creation of too large blocks, signatures are further divided
along their first given name initial.  Cases 4 and 5 are not
explicitly handled.

\glnote{I am not sure we need an explicit algorithm, but adding a few examples would help.}

\subsection{Linkage function}
\label{methods:linkage}

\textit{Supervised classification.} The second part of the algorithm is the
automatic construction of a pair-wise linkage function between signatures, for
later reuse during clustering to group all signatures from a same author.
Formally, the goal is to build a function $d: {\cal S} \times {\cal S} \mapsto
[0, 1]$, such that $d(s_1, s_2)$ tends to $0$ if both signatures $s_1$ and
$s_2$ belong to the same author, and $1$ otherwise.

As in previous works on author disambiguation, we cast this problem as a
classification task, where inputs are pairs of signatures and outputs are
classes $0$ (same authors) and $1$ (distinct authors).

\textit{Features engineering.}

\textit{Building a training set.}
% cite https://hpi.de/naumann/projects/repeatability/datasets/dblp-dataset.html

% distance learning as a supervised problem
% describe all features
% balancing difficult and easy cases
% sampling pairs from block only

\subsection{Semi-supervised clustering}
\label{methods:clustering}
\glnote{By Gilles}


% hierarchical clustering
% semi-supervised clustering
% cold start problem (when no labels in a block)

% \subsection{Matching}
% general description in a real system
% GL: out of scope, let us mention that in Conclusions instead


% Implementation ======================================================

\section{Implementation}
\label{implementation}

As part of this work, we developed a stand-alone application for author
disambiguation, publicly available
online\footnote{\url{https://github.com/inveniosoftware/beard}} for free reuse
or study.  Our implementation builds upon the Python scientific stack, making
use of the Scikit-Learn library \citep{scikitlearn} for the supervised learning
of a linkage function and of SciPy \citep{scipy} for clustering. All
components of the disambiguation pipeline have been designed to follow the
Scikit-Learn API \citep{scikitlearnAPI}, making them easy to maintain,
understand and reuse.

\glnote{Not sure we should talk about parallelization?}
\glnote{Mention (future) deployment in Inspire?}

% Performance-wise, the clustering step appeared to be the bottleneck. We improved the
% algorithmic efficiency by parallelizing it. Having $n$ cores available, one can run the
% semi-supervised clustering for $n$ different blocks in the same moment. In the production
% systems the disambiguation often need not be run on the whole dataset. Once the dataset has
% been disambiguated once, it is possible to avoid rerunning the whole algorithm when the new
% signatures arrive in the digital library. First of all, the similarity function can be
% reused. Moreover, the clustering has to be applied only to the blocks the new signatures are
% contained within.


% Results and discussion ======================================================

\section{Experiments}
\label{experiments}

\subsection{Data}

\glnote{By Gilles}

% source of the data, labels in the data, etc
% release the data

\subsection{Evaluation protocol}

\glnote{By Gilles}

% measures (b^3, paired, size of clusters)
% training / valid / test sets

\subsection{Base experiment}

\glnote{To review}

The starting point of our analysis is an algorithm that blocks the signatures using the last
name and first initial strategy, samples the pairs in balanced way, uses gradient boosting
with all features included for learning the similarity function and clusters using
semi-supervised hierarchical approach.

\begin{table}[H]
\caption{Base experiment evaluation)}
\centering
\begin{tabular}{|r|r|l|l|l|}
  \hline
  Evaluation & Set & Precision & Recall & F-score \\
  \hline
  Pairwise & test & 0.9948 & 0.9738 & 0.9842 \\
  \hline
  Pairwise & train & 0.9976 & 0.9747 & 0.986 \\
  \hline
  Pairwise & overall & 0.9952 & 0.9739 & 0.9844 \\
  \hline
  $B^3$ & test & 0.9901 & 0.976 & 0.983 \\
  \hline
  $B^3$ & train & 0.997 & 0.979 & 0.9879 \\
  \hline
  $B^3$ & overall & 0.9907 & 0.9761 & 0.9834 \\
  \hline
\end{tabular}
\end{table}

\subsection{Blocking}

\glnote{To review}

Let us introduce a term \textit{blocking limited maximum recall} to describe the highest
recall that the algorithm can theoretically reach after blocking is done.
Formally, for the set of signatures ${\cal S}$, blocking function $F: \mathcal{S}
\rightarrow \mathcal{B}$, where $ \mathcal{B} = \{s_{0}, \ldots{} , s_{N-1}\}$ is a
partition of $\mathcal{S} $ , the recall function $\mathcal{R}: \mathcal{C}_{gold},
\mathcal{C}_{predicted} \rightarrow \langle0,1\rangle$ where $\mathcal{C}_{gold}$ is the
partition of $\mathcal{S}$ where for every author there is a set in $\mathcal{C}_{gold}$
that contains only all the signatures of this author, $\mathcal{C}_{predicted}$ is the
partition of $\mathcal{S}$ output by the algorithm, and $\mathcal{Q} = \{\cup_{i=0}^{N-1}
q_i $ : $\forall_{i \in \langle0,N-1\rangle} q_i$ is a partition of $s_i$\}. Then
\textit{blocking limited maximum recall} is defined as $\max_{q \in \mathcal{Q}}
\mathcal{R}(\mathcal{C}_{gold}, q)$. For sanely defined recall functions, such as $B^{3}$
recall or \textit{pairwise} recall \msnote{Further explanation needed, why it is
equivalent?}, it is equivalent to $\mathcal{R}(\mathcal{C}_{gold}, \mathcal{B})$.

The \textit{blocking limited maximum recall} for the last name and first initial blocking
and $B^3$ scoring equals 0.9827. Comparing it to the $B^3$ recall of the base experiment,
one can see that a huge part of errors comes from the blocking step which forbids some of
the pairs to be merged together. To prevent that, we tried the phonetic blocking algorithm
using three different phonetic algorithms: Double Metaphone \citep{doublemetaphone}, NYSIIS
\citep{nysiis} and Soundex \citep{Soundex}. As Double Metaphone can return two results for a
string, we always discarded the second one when we used this algorithm.

\begin{table}[H]
\caption{Blocking limited maximum recalls (BLMR) on the whole dataset without splitting over
the first given name initial}
\centering
\begin{tabular}{|r|l|l|}
  \hline
  blocking function & BLMR with $B^{3}$ recall & BLMR with pairwise recall \\
  \hline
  last name and first initial & 0.9827 & 0.9776 \\
  \hline
  phonetic using double metaphone & 0.9967 & 0.9953 \\
  \hline
  phonetic using nysiis & 0.9961 & 0.9951 \\
  \hline
  phonetic using soundex & 0.9966 & 0.9953 \\
  \hline
\end{tabular}
\end{table}

\msnote{Mention two new features for this experiments, other than that everything is the
same as in the base experiment}

\begin{table}[H]
\caption{Phonetic algorithm performance}
\centering
\begin{tabular}{|r|r|r|l|l|l|}
  \hline
  Phonetic Algorithm & Split over initial & Evaluation & Precision & Recall & F-score \\
  \hline
  Double Metaphone & yes & pairwise & 0.9934 & 0.982 & 0.9876 \\
  \hline
  Double Metaphone & yes & $B^3$ & 0.9867 & 0.983 & 0.9848 \\
  \hline
  Double Metaphone & no & pairwise & - & - & - \\
  \hline
  Double Metaphone & no & $B^3$ & - & - & - \\
  \hline
  NYSIIS & yes & pairwise & 0.9942 & 0.9818 & 0.9879 \\
  \hline
  NYSIIS & yes & $B^3$ & 0.9884 & 0.9828 & 0.9856 \\
  \hline
  NYSIIS & no & pairwise & - & - & - \\
  \hline
  NYSIIS & no & $B^3$ & - & - & - \\
  \hline
  Soundex & yes & pairwise & - & - & - \\
  \hline
  Soundex & yes & $B^3$ & - & - & - \\
  \hline
  Soundex & no & pairwise & - & - & - \\
  \hline
  Soundex & no & $B^3$ & - & - & - \\
  \hline
\end{tabular}
\end{table}

\msnote{Fill the table}

% compare with torvik & smalheiser (2009) who argue that lnfi blocking has recall around 98%
% MS - recall for lnfi (blocking only) 0.9776, lnfi (whole algorithm) 0.9739,
% MS - all three results use pairwise scoring

\subsection{Learning a linkage function}
% feature engineering
% variable importance analysis
% supervised learning model

\subsection{Semi-supervised clustering}
% compare agglomerative algorithms
% compare threshold strategies
% compare cold start solutions


% Conclusions =================================================================

\section{Conclusions}
\label{conclusions}

% future improvements


% References ==================================================================

\bibliographystyle{apalike}
\bibliography{bib}

\end{document}
