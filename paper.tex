\documentclass{article}
%\usepackage[margin=1.2in]{geometry}

\usepackage{nips15submit_e}


% For figures
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% For links
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}


% Header ======================================================================

\title{Author disambiguation of large-scale semi-labeled collections of scientific articles}

\author{Gilles Louppe\\
        CERN\\
        Switzerland\\
\And Hussein Al-Natsheh\\
        CERN\\
        Switzerland\\
\And Mateusz Susik\\
        CERN\\
        Switzerland\\
\And Eamonn Maguire\\
        CERN\\
        Switzerland}
\date{}

\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}

Author name disambiguation in bibliographic databases is the problem of
grouping together scientific publications written by a same person, accounting
for potential homonyms and/or synonyms. Among solutions to this problem, digital
libraries are increasingly offering tools for authors to manually curate their
publications and claim which ones are theirs. Indirectly, these tools allow for
the inexpensive collection of large annotated training data, which can be
further leveraged to build an automated disambiguation system capable of
inferring patterns for identifying publications written by a same
person.  In this context, we propose a semi-supervised machine learning
disambiguation solution in the case large and partially annotated databases of
scientific publications. Using a real-world dataset of 1 million manual
annotations, we demonstrate how we exploited this data (i) to learn an accurate
classifier for identifying corefering authors, (ii) to guide the clustering of
scientific publications by distinct authors in a semi-supervised way and
finally (iii) to match the resulting clusters with existing profiles.  Our
analysis on this large training set reveal that...
\glnote{To be completed with actual results.}

\end{abstract}


% Introduction ==================================================================

\section{Introduction}
\label{introduction}

% general introduction to the disambiguation issues
% often based on small dataset or fully unsupervised
% summary of what we did
% outline of the paper

In academic digital libraries, author name disambiguation is the problem of
grouping together publications written by a same person.  Author name
disambiguation is often a difficult problem because an author may use different
spellings or name variants across her career (synonymy) and/or distinct authors may
share the same name (polysemy). Most notably, author disambiguation is often even more
troublesome in the case of researchers from non-Western cultures, where
personal names may be traditionally less diverse (leading to homonym issues) or
for which transliteration to latin characters may not be unique (leading to
synonym issues). With the fast growth of the scientific literature, author
disambiguation has become a pressing issue because the accuracy of information
managed at the level of individuals directly affects the relevance search
results (e.g., when querying for all publications written by a given author),
the reliability of bibliometrics and author rankings (e.g., citation counts or other impact
metrics, as studied in \citep{strotmann2012author}) or the relevance of scientific network analysis.

Efforts and solutions to author disambiguation have been proposed from various
communities \citep{liu2014author}. On the one hand, libraries have maintained
authorship control through manual curation, either in a centralized way by
hiring collaborators or by developing services inviting authors to register
their publications themselves (e.g., Google Scholar, Inspire-HEP, ...). Recently,
efforts also started to create persistent digital identifiers assigned to researchers
(e.g., ORCID, ResearchedID, ...), with the further objective to embed these
identifiers in the submission workflow of publishers or repositeries (e.g.,
Elsevier, arXiv, Inspire-HEP, ...), thereby univocally solving any disambiguation
issue. With the large cost of centralized manual authorship control or until
the larger adoption of crowdsourced solutions, the impact of these efforts are
unfortunately limited by the efficiency, motivation or integrity of their
active contributors. Similarly, the success of persistent digital identifier
efforts is conditioned to a large and ubiquituous adoption by both researchers and publishers.
For these reasons, fully automated machine learning-based methods have been
proposed during the past decade to provide immediate, less costly
and efficient solutions to author disambiguation. In this work, our goal
is to explore and demonstrate how both approaches can coexist and benefit from
each other.  In particular, we study how labeled data obtained through manual
curation (either centralized or crowdsourced) can be exploited (i) to learn an
accurate classifier for identifying corefering authors, (ii) to guide the
clustering of scientific publications by distinct authors in a semi-supervised
way and finally (iii) to match the resulting clusters with existing profiles.
Our analysis of parameters and features on this large training set reveal
that... \glnote{To be completed with actual results.}

The remaining of this report is structured as follows. In Section~\ref{related-works}, ...


% Related works ==================================================================

\section{Related works}
\label{related-works}

As reviewed in
\citep{smalheiser2009author,ferreira2012brief,levin2012citation}, author
disambiguation algorithms are usually composed of two components: (i) a
similarity function determining whether two publications have been written by a
same author and (ii) a clustering algorithm producing clusters of publications
assumed to be written by a same author. Approaches can be inventorized along
several axes, depending on the type and amount of data available, the way the
similarity function is learned or defined, or the clustering procedure used to
group publications. Methods relying on supervised learning usually make use of
hand-labeled pairs of publications identified as being either from the same or
from different authors for learning automatically a similarity function between
publications \citep{han2004two,huang2006efficient,
culotta2007author,treeratpituk2009disambiguating,tran2014author}. Because
training data is often not easily available, unsupervised approaches propose
instead to use domain-specific similarity functions tailored for author
disambiguation \citep{malin2005unsupervised,mcrae2006also,song2007efficient,
soler2007separating, kang2009co,fan2011graph,schulz2014exploiting}. These later
approaches have the advantage of not requiring  hand-labeled data, but often do
not perform as well as the supervised approaches. To reconcile both worlds,
semi-supervised methods make use of small, manually verified, clusters of
publications and/or of high-precision domain-specific rules to build a training
set of pairs of publications, from which a similarity function is then built
using supervised learning
\citep{ferreira2010effective,torvik2009author,levin2012citation}.
Coincidentally, semi-supervised approaches also allow for the tuning of the
clustering algorithm when the later is applied to a mixed set of labeled
and unlabeled publications, e.g., by maximizing some clustering performance
metric on the known clusters \citep{levin2012citation}.

Because of the lack of a large and publicly available dataset of curated
clusters of publications, studies on author disambiguation are usually
constrained to validate their results on manually built datasets of limited
size and scope (from a few hundreds to a few thousands of papers, with a sparse
coverage of ambiguous cases), making the true performance of these methods
often difficult to assess with high confidence. In addition, despite devoted
efforts to construct them, these datasets are very rarely released publicly,
making it even more difficult to compare methods on a common benchmark.

In this context, we position the work presented in this paper as a
semi-supervised  solution for author disambiguation, but with the significant
advantage of having a very large collection of more than 1 million annotations
of publications known to be from the same author. In particular, the extent and coverage
of this data allows us to revisit, validate and nuance previous findings regarding
supervised learning of similarity functions.



% Methods =====================================================================

\section{Semi-supervised author disambiguation}
\label{methods}

\subsection{Overview}

Formally, let us assume a set of publications ${\cal P} = \{ p_0, ...,
p_{N-1}\}$ along with the set of unique individuals ${\cal A} = \{ a_0, ...,
a_{M-1}\}$ having together authored all publications in ${\cal P}$.  Let us
define a signature $s \in p$ from a publication as a unique piece of
information identifying one of the authors of $p$ (e.g., the author name, his
affiliation, along with any other metadata that can be derived from $p$). Let us
denote by ${\cal S} = \{ s | s \in p, p \in {\cal P} \}$ the set of all
signatures that can be extracted from all publications in ${\cal P}$. In this
framework, author disambiguation can be stated as the problem of
finding a partition ${\cal C} = \{ c_0, ..., c_{M-1} \}$ of ${\cal S}$ such
that ${\cal S} = \cup_{i=0}^{M-1} c_i$, $c_i \cap c_j = \phi$ for all $i \neq
j$, and where subsets $c_i$, or clusters, each corresponds to the set of all
signatures belonging to the same individual $a_i$. Alternatively, the set
${\cal A}$ may remain (possibly partially) unknown, such that author
disambiguation boils down to finding a partition ${\cal C}$ where
subsets $c_i$  each corresponds to the set of all signatures from a same
individual. Finally, in the case of partially annotated databases as studied in
this work, the setting extends with the partial knowledge of pairs $(a, s) \in
{\cal A} \times {\cal S}$, each indicating that signature $s$ is known to
belong to individual $a$.

In this work, author disambiguation is cast into a semi-supervised clustering
problem, as inspired from several previous works described in Section~\ref{related-works}.
Our algorithm is made of four elements: ...

% Our methodology consists in 4 main components: Distance learning, blocking,
% semi-supervised clustering and matching. A distance function is learned from a
% training set of signiture pairs sampled from ground-truth data. Each pair is
% labeled whether the 2 signatures are for the same real author or not. The
% learned distance function will then be used for computing the dissimilarity
% matrix of a block of signatures. A block of signatures is grouping signatures
% that have in common similar author names, for example same last name. This
% affinity matrix would be the main input of our semi-supervised clustering
% algorithm. The predicted clusters of each block will lastly be combined into a
% final predicted clusters result. Each cluster label represents the predicted
% single author of the publications in the cluster. Finally, we present a
% strategy for matching the predicted results with the publications in which
% their authors were claimed.

\subsection{Blocking}
% motivation
% last name, first initial
% phonetics something (dm)

\subsection{Distance learning}
% distance learning as a supervised problem
% balancing difficult and easy cases

\subsection{Semi-supervised clustering}
% hierarchical clustering
% semi-supervised clustering
% cold start problem (when no labels in a block)

\subsection{Matching}
% general description in a real system


% Implementation ======================================================

\section{Implementation}
\label{implementation}


% Results and discussion ======================================================

\section{Experiments}
\label{experiments}

\subsection{Data}
% source of the data, labels in the data, etc
% release the data

\subsection{Evaluation protocol}
% measures (b^3, paired, size of clusters)
% training / valid / test sets

\subsection{Blocking}

\subsection{Distance learning}
% feature engineering
% variable importance analysis
% supervised learning model

\subsection{Semi-supervised clustering}
% compare agglomerative algorithms
% compare threshold strategies
% compare cold start solutions
% talk about impact of small improvements in terms of citations statistics


% Conclusions =================================================================

\section{Conclusions}
\label{conclusions}

% future improvements


% References ==================================================================

\bibliographystyle{apalike}
\bibliography{bib}

\end{document}
